"""
ç…¤çŸ¿ç“¦æ–¯é£é™©é¢„æµ‹ç³»ç»Ÿ - æ¨¡å‹è®­ç»ƒæ¨¡å—
åŒ…å«ï¼šå…¨é‡è®­ç»ƒã€å¢é‡è®­ç»ƒã€è®­ç»ƒæµç¨‹æ§åˆ¶
ä¾èµ–ï¼šlightgbmã€scikit-learn
"""
import lightgbm as lgb
import xgboost as xgb
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from loguru import logger
from sklearn.model_selection import train_test_split

from config_utils import error_handler_decorator


class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, config):
        self.config = config
        self.algorithm = config.get("Model", "algorithm", fallback="lightgbm")
        self.n_estimators = config.getint("Model", "n_estimators", fallback=100)
        self.increment_estimators = config.getint("Model", "increment_estimators", fallback=30)
        # è¯»å–é€šç”¨å‚æ•°
        self.learning_rate = config.getfloat("Model", "learning_rate", fallback=0.05)  # å­¦ä¹ ç‡ï¼ˆæµ®ç‚¹æ•°ï¼‰
        # è¯»å–LightGBMæ ¸å¿ƒå‚æ•°ï¼ˆä»config.iniï¼‰
        self.num_leaves = config.getint("Model", "num_leaves", fallback=31)  # å¶å­èŠ‚ç‚¹æ•°ï¼ˆæ•´æ•°ï¼‰
        self.reg_alpha = config.getfloat("Model", "reg_alpha", fallback=0.0)  # L1æ­£åˆ™åŒ–ï¼ˆæµ®ç‚¹æ•°ï¼‰
        self.reg_lambda = config.getfloat("Model", "reg_lambda", fallback=0.0)  # L2æ­£åˆ™åŒ–ï¼ˆæµ®ç‚¹æ•°ï¼‰

        # XGBoostç‰¹å®šå‚æ•°
        self.max_depth = config.getint("Model", "max_depth", fallback=6)
        self.subsample = config.getfloat("Model", "subsample", fallback=0.8)
        self.colsample_bytree = config.getfloat("Model", "colsample_bytree", fallback=0.8)

        # ä»é…ç½®è¯»å–è¿‡æ‹Ÿåˆåˆ¤æ–­é˜ˆå€¼
        self.overfitting_threshold = config.getfloat("Model", "overfitting_threshold", fallback=1.5)
        self.underfitting_large_threshold = config.getfloat("Model", "underfitting_large_threshold", fallback=0.9)
        self.underfitting_small_threshold = config.getfloat("Model", "underfitting_small_threshold", fallback=1.2)
        # å­˜å‚¨è®­ç»ƒè¯Šæ–­ä¿¡æ¯
        self.last_training_details = {}
        logger.info(f"æ¨¡å‹è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨ç®—æ³•: {self.algorithm}")

    def _get_model_params(self):
        """è·å–å½“å‰ç®—æ³•çš„å‚æ•°é…ç½®"""
        if self.algorithm == "lightgbm":
            return {
                'objective': 'regression',
                'metric': 'rmse',
                'learning_rate': self.learning_rate,
                'num_leaves': self.num_leaves,
                'reg_alpha': self.reg_alpha,
                'reg_lambda': self.reg_lambda,
                'verbosity': -1,
                'force_col_wise': True
            }
        elif self.algorithm == "xgboost":
            return {
                'objective': 'reg:squarederror',
                'learning_rate': self.learning_rate,
                'max_depth': self.max_depth,
                'subsample': self.subsample,
                'colsample_bytree': self.colsample_bytree,
                'verbosity': 0
            }
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {self.algorithm}")

    def _train_model(self, params, train_data, num_round, val_data=None, init_model=None):
        """é€šç”¨æ¨¡å‹è®­ç»ƒæ–¹æ³•"""
        if self.algorithm == "lightgbm":
            if val_data:
                return lgb.train(
                    params,
                    train_data,
                    num_boost_round=num_round,
                    valid_sets=[train_data, val_data],
                    valid_names=['train', 'val'],
                    init_model=init_model,
                    callbacks=[lgb.log_evaluation(0)]
                )
            else:
                return lgb.train(
                    params,
                    train_data,
                    num_boost_round=num_round,
                    init_model=init_model,
                    callbacks=[lgb.log_evaluation(0)]
                )
        elif self.algorithm == "xgboost":
            eval_set = [(val_data, 'val')] if val_data else []
            return xgb.train(
                params,
                train_data,
                num_boost_round=num_round,
                evals=eval_set,
                xgb_model=init_model,
                verbose_eval=0
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {self.algorithm}")

    def _create_dataset(self, X, y):
        """åˆ›å»ºç®—æ³•ç‰¹å®šçš„æ•°æ®é›†"""
        if self.algorithm == "lightgbm":
            return lgb.Dataset(X, label=y)
        elif self.algorithm == "xgboost":
            return xgb.DMatrix(X, label=y)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {self.algorithm}")

    def _predict_model(self, model, X):
        """æ¨¡å‹é¢„æµ‹æ–¹æ³•"""
        if self.algorithm == "lightgbm":
            return model.predict(X)
        elif self.algorithm == "xgboost":
            if isinstance(X, xgb.DMatrix):
                return model.predict(X)
            else:
                return model.predict(xgb.DMatrix(X))
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {self.algorithm}")

    def _get_tree_count(self, model):
        """è·å–æ¨¡å‹æ ‘æ•°é‡"""
        if self.algorithm == "lightgbm":
            return model.num_trees()
        elif self.algorithm == "xgboost":
            return len(model.get_dump())
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {self.algorithm}")

    @error_handler_decorator
    def _full_train(self, X_proc, y, target_features):
        """
        ç§æœ‰æ–¹æ³•ï¼šå…¨é‡è®­ç»ƒï¼ˆæ¸…ç©ºç°æœ‰æ¨¡å‹ï¼Œé‡æ–°è®­ç»ƒæ‰€æœ‰ç›®æ ‡ï¼‰

        :param X_proc: é¢„å¤„ç†åçš„ç‰¹å¾æ•°æ®
        :param y: ç›®æ ‡å€¼
        :param target_features: ç›®æ ‡ç‰¹å¾åˆ—è¡¨
        :return: dictï¼Œè®­ç»ƒå¥½çš„æ¨¡å‹å­—å…¸
        """
        # æ•°æ®æ ¡éªŒ
        if X_proc is None or y is None:
            raise ValueError("è®­ç»ƒæ•°æ®X_procæˆ–yä¸èƒ½ä¸ºNone")
        if len(X_proc) != len(y):
            raise ValueError(f"ç‰¹å¾ä¸ç›®æ ‡æ•°æ®é•¿åº¦ä¸åŒ¹é…ï¼š{len(X_proc)} vs {len(y)}")
        if not target_features:
            raise ValueError("ç›®æ ‡ç‰¹å¾åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        models = {}
        training_details = {
            'target_performance': {},
            'overfitting_diagnosis': {},
            'parameter_suggestions': [],
            'algorithm': self.algorithm
        }

        params = self._get_model_params()

        for i, target in enumerate(target_features):
            logger.info(f"å…¨é‡è®­ç»ƒå¼€å§‹ â†’ ç®—æ³•: {self.algorithm}, ç›®æ ‡: {target}, é¢„æœŸæ ‘æ•°é‡: {self.n_estimators}")

            # ä½¿ç”¨äº¤å‰éªŒè¯æ€è·¯ï¼Œä½†ä¸å‡å°‘è®­ç»ƒæ•°æ®
            if len(X_proc) > 20:  # æ•°æ®é‡è¶³å¤Ÿæ—¶åˆ’åˆ†éªŒè¯é›†
                X_train, X_val, y_train, y_val = train_test_split(
                    X_proc, y[:, i], test_size=0.2, random_state=42
                )
                use_validation = True
            else:
                # æ•°æ®é‡å°‘æ—¶ä½¿ç”¨å…¨éƒ¨æ•°æ®
                X_train, y_train = X_proc, y[:, i]
                X_val, y_val = X_proc, y[:, i]  # ç”¨è®­ç»ƒæ•°æ®åš"éªŒè¯"
                use_validation = False

            train_data = self._create_dataset(X_train, y_train)
            val_data = self._create_dataset(X_val, y_val) if use_validation else None

            # è®­ç»ƒæ¨¡å‹
            if use_validation:
                model = self._train_model(
                    params, train_data, self.n_estimators, val_data
                )
            else:
                model = self._train_model(
                    params, train_data, self.n_estimators
                )
            models[target] = model

            # éªŒè¯å®é™…æ ‘æ•°é‡æ˜¯å¦ä¸é¢„æœŸä¸€è‡´
            actual_trees = self._get_tree_count(model)
            if actual_trees != self.n_estimators:
                logger.warning(f"ç›®æ ‡ {target} å®é™…æ ‘æ•°é‡ ({actual_trees}) ä¸é¢„æœŸ ({self.n_estimators}) ä¸ä¸€è‡´")

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡ï¼ˆä¿å®ˆè¯Šæ–­ï¼‰
            train_pred = self._predict_model(model, X_train)
            val_pred = self._predict_model(model, X_val) if use_validation else train_pred

            train_rmse = np.sqrt(np.mean((y_train - train_pred) ** 2))
            val_rmse = np.sqrt(np.mean((y_val - val_pred) ** 2))

            # è¿‡æ‹Ÿåˆè¯Šæ–­ï¼ˆä¿å®ˆåˆ¤æ–­ï¼‰
            if use_validation:
                overfitting_ratio = val_rmse / train_rmse if train_rmse > 0 else 1.0
                is_overfitting = overfitting_ratio > self.overfitting_threshold
                is_underfitting = train_rmse > np.std(y_train) * self.underfitting_large_threshold
            else:
                # æ•°æ®é‡ä¸è¶³æ—¶ï¼Œä¸è¿›è¡Œè¿‡æ‹Ÿåˆè¯Šæ–­
                overfitting_ratio = 1.0
                is_overfitting = False
                is_underfitting = train_rmse > np.std(y_train) * self.underfitting_small_threshold

            training_details['target_performance'][target] = {
                'train_rmse': round(train_rmse, 4),
                'val_rmse': round(val_rmse, 4),
                'overfitting_ratio': round(overfitting_ratio, 2),
                'is_overfitting': is_overfitting,
                'is_underfitting': is_underfitting,
                'trees_count': actual_trees,
                'expected_trees': self.n_estimators,
                'use_validation': use_validation
            }

            logger.info(
                f"å…¨é‡è®­ç»ƒå®Œæˆ â†’ ç®—æ³•: {self.algorithm}, ç›®æ ‡: {target}, "
                f"è®­ç»ƒRMSE: {train_rmse:.4f}, éªŒè¯RMSE: {val_rmse:.4f}, "
                f"é¢„æœŸæ ‘æ•°é‡: {self.n_estimators}, å®é™…æ ‘æ•°é‡: {actual_trees}"
            )

        # ç”Ÿæˆå‚æ•°è°ƒæ•´å»ºè®®
        training_details['parameter_suggestions'] = self._generate_parameter_suggestions(
            training_details['target_performance'],
            getattr(self, 'config_filename', 'config.ini')
        )

        # å­˜å‚¨è¯Šæ–­ä¿¡æ¯ä¾›å¤–éƒ¨è®¿é—®
        self.last_training_details = training_details

        return models

    @error_handler_decorator
    def _incremental_train(self, X_proc, y, target_features, existing_models):
        """
        ç§æœ‰æ–¹æ³•ï¼šå¢é‡è®­ç»ƒï¼ˆåŸºäºç°æœ‰æ¨¡å‹è¿½åŠ è®­ç»ƒï¼‰

        :param X_proc: é¢„å¤„ç†åçš„ç‰¹å¾æ•°æ®
        :param y: ç›®æ ‡å€¼
        :param target_features: ç›®æ ‡ç‰¹å¾åˆ—è¡¨
        :param existing_models: ç°æœ‰æ¨¡å‹å­—å…¸
        :return: dictï¼Œæ›´æ–°åçš„æ¨¡å‹å­—å…¸
        """
        if not existing_models:
            raise ValueError("å¢é‡è®­ç»ƒéœ€è¦éç©ºçš„ç°æœ‰æ¨¡å‹å­—å…¸")
        if X_proc is None or y is None:
            raise ValueError("è®­ç»ƒæ•°æ®X_procæˆ–yä¸èƒ½ä¸ºNone")
        if len(X_proc) == 0:
            raise ValueError("è®­ç»ƒæ•°æ®æ ·æœ¬æ•°é‡ä¸èƒ½ä¸º0")

        params = self._get_model_params()
        training_details = {
            'target_performance': {},
            'overfitting_diagnosis': {},
            'parameter_suggestions': [],
            'algorithm': self.algorithm
        }

        for i, target in enumerate(target_features):
            init_model = existing_models.get(target)
            if init_model is None:
                raise ValueError(f"å¢é‡è®­ç»ƒå¤±è´¥ï¼šç›®æ ‡ {target} æ— ç°æœ‰æ¨¡å‹")

            # æ£€æŸ¥å¹¶æ‰“å°æ¨¡å‹çŠ¶æ€
            initial_trees = self._get_tree_count(init_model) if init_model else 0
            logger.info(
                f"å¢é‡è®­ç»ƒå¼€å§‹ â†’ ç®—æ³•: {self.algorithm}, ç›®æ ‡: {target}, å½“å‰æ ‘æ•°é‡: {initial_trees}, è®¡åˆ’è¿½åŠ : {self.increment_estimators}")

            # å¦‚æœç°æœ‰æ¨¡å‹æ ‘æ•°é‡å¼‚å¸¸ï¼ˆâ‰¤1ï¼‰ï¼Œä½¿ç”¨ n_estimators è¿›è¡Œå…¨é‡è®­ç»ƒè€Œä¸æ˜¯ increment_estimators
            if initial_trees <= 1:
                logger.warning(f"ç›®æ ‡ {target} çš„ç°æœ‰æ¨¡å‹æ ‘æ•°é‡å¼‚å¸¸ï¼ˆ{initial_trees}ï¼‰ï¼Œä½¿ç”¨å…¨é‡è®­ç»ƒä»£æ›¿å¢é‡è®­ç»ƒï¼ˆn_estimators={self.n_estimators}ï¼‰")
                # ä½¿ç”¨å…¨é‡è®­ç»ƒé€»è¾‘ï¼Œä½†ä½¿ç”¨å½“å‰æ•°æ®
                if len(X_proc) > 20:
                    X_train, X_val, y_train, y_val = train_test_split(
                        X_proc, y[:, i], test_size=0.2, random_state=42
                    )
                    use_validation = True
                else:
                    X_train, y_train = X_proc, y[:, i]
                    X_val, y_val = X_proc, y[:, i]
                    use_validation = False

                train_data = self._create_dataset(X_train, y_train)
                val_data = self._create_dataset(X_val, y_val) if use_validation else None

                # ä½¿ç”¨ n_estimators è€Œä¸æ˜¯ increment_estimators è¿›è¡Œå…¨é‡è®­ç»ƒ
                if use_validation:
                    model = self._train_model(
                        params, train_data, self.n_estimators, val_data
                    )
                else:
                    model = self._train_model(
                        params, train_data, self.n_estimators
                    )
                existing_models[target] = model
                final_trees = self._get_tree_count(model)
                actual_increment = final_trees  # å› ä¸ºæ˜¯å…¨é‡è®­ç»ƒï¼Œæ‰€ä»¥å¢é‡ç­‰äºæœ€ç»ˆæ ‘æ•°é‡
            else:
                # æ­£å¸¸çš„å¢é‡è®­ç»ƒé€»è¾‘
                train_data = self._create_dataset(X_proc, y[:, i])
                model = self._train_model(
                    params, train_data, self.increment_estimators, init_model=init_model
                )
                existing_models[target] = model

                # éªŒè¯æ ‘æ•°é‡æ˜¯å¦æ­£ç¡®ç´¯åŠ 
                final_trees = self._get_tree_count(model)
                actual_increment = final_trees - initial_trees

            logger.info(
                f"å¢é‡è®­ç»ƒå®Œæˆ â†’ ç®—æ³•: {self.algorithm}, ç›®æ ‡: {target}, è¿½åŠ æ ‘æ•°é‡: {actual_increment}, ç´¯è®¡æ ‘æ•°é‡: {final_trees}")

            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡ï¼ˆä½¿ç”¨è®­ç»ƒæ•°æ®ï¼‰
            train_pred = self._predict_model(model, X_proc)
            train_rmse = np.sqrt(np.mean((y[:, i] - train_pred) ** 2))

            # å¢é‡è®­ç»ƒä¸è¿›è¡Œè¿‡æ‹Ÿåˆè¯Šæ–­ï¼ˆæ•°æ®ä¸è¶³ï¼‰
            training_details['target_performance'][target] = {
                'train_rmse': round(train_rmse, 4),
                'val_rmse': round(train_rmse, 4),  # ä¸è®­ç»ƒé›†ç›¸åŒ
                'overfitting_ratio': 1.0,
                'is_overfitting': False,
                'is_underfitting': train_rmse > np.std(y[:, i]) * 1.2,
                'trees_count': final_trees,
                'incremental_trees': actual_increment,
                'use_validation': False,
                'initial_trees': initial_trees,
                'train_mode': 'full' if initial_trees <= 1 else 'incremental'  # è®°å½•è®­ç»ƒæ¨¡å¼
            }

            logger.info(
                f"ç›®æ ‡ {target} â†’ è®­ç»ƒæ¨¡å¼: {'å…¨é‡è®­ç»ƒ' if initial_trees <= 1 else 'å¢é‡è®­ç»ƒ'}, "
                f"è¿½åŠ æ ‘æ•°é‡: {actual_increment}, "
                f"ç´¯è®¡æ ‘æ•°é‡: {final_trees}, "
                f"è®­ç»ƒRMSE: {train_rmse:.4f}"
            )

        # ç”Ÿæˆå‚æ•°è°ƒæ•´å»ºè®®
        training_details['parameter_suggestions'] = self._generate_parameter_suggestions(
            training_details['target_performance'],
            getattr(self, 'config_filename', 'config.ini')
        )

        # å­˜å‚¨è¯Šæ–­ä¿¡æ¯
        self.last_training_details = training_details

        return existing_models

    def _generate_parameter_suggestions(self, target_performance, config_filename="config.ini"):
        """
        ç”Ÿæˆå‚æ•°è°ƒæ•´å»ºè®®åŸºäºæ¨¡å‹æ€§èƒ½è¯Šæ–­

        :param target_performance: å„ç›®æ ‡æ€§èƒ½æŒ‡æ ‡
        :return: list, å‚æ•°è°ƒæ•´å»ºè®®
        """
        suggestions = []

        algorithm_specific_advice = {
            'lightgbm': [
                "  â€¢ è°ƒæ•´ num_leavesï¼ˆå½“å‰å€¼ï¼š{}ï¼‰æ§åˆ¶æ¨¡å‹å¤æ‚åº¦".format(self.num_leaves),
                "  â€¢ è°ƒæ•´ reg_alphaï¼ˆå½“å‰å€¼ï¼š{}ï¼‰æ§åˆ¶L1æ­£åˆ™åŒ–".format(self.reg_alpha),
                "  â€¢ è°ƒæ•´ reg_lambdaï¼ˆå½“å‰å€¼ï¼š{}ï¼‰æ§åˆ¶L2æ­£åˆ™åŒ–".format(self.reg_lambda)
            ],
            'xgboost': [
                "  â€¢ è°ƒæ•´ max_depthï¼ˆå½“å‰å€¼ï¼š{}ï¼‰æ§åˆ¶æ ‘æ·±åº¦".format(self.max_depth),
                "  â€¢ è°ƒæ•´ subsampleï¼ˆå½“å‰å€¼ï¼š{}ï¼‰æ§åˆ¶æ ·æœ¬é‡‡æ ·".format(self.subsample),
                "  â€¢ è°ƒæ•´ colsample_bytreeï¼ˆå½“å‰å€¼ï¼š{}ï¼‰æ§åˆ¶ç‰¹å¾é‡‡æ ·".format(self.colsample_bytree)
            ]
        }

        # ç»Ÿè®¡è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆæƒ…å†µ
        overfitting_targets = [t for t, perf in target_performance.items()
                               if perf.get('is_overfitting', False)]
        underfitting_targets = [t for t, perf in target_performance.items()
                                if perf.get('is_underfitting', False)]

        if overfitting_targets:
            suggestions.extend([
                f"ğŸ“ˆ æ£€æµ‹åˆ°è¿‡æ‹Ÿåˆç°è±¡ï¼Œå»ºè®®è°ƒæ•´{algorithm_specific_advice.get(self.algorithm, [])}:"
            ])
            suggestions.extend(algorithm_specific_advice.get(self.algorithm, []))
            suggestions.append("  â€¢ å‡å° learning_rateï¼ˆå½“å‰å€¼ï¼š{}ï¼‰å¹¶å¢åŠ  n_estimators".format(self.learning_rate))

        if underfitting_targets:
            suggestions.extend([
                f"ğŸ“‰ æ£€æµ‹åˆ°æ¬ æ‹Ÿåˆç°è±¡ï¼Œå»ºè®®è°ƒæ•´{algorithm_specific_advice.get(self.algorithm, [])}:"
            ])
            suggestions.extend(algorithm_specific_advice.get(self.algorithm, []))
            suggestions.extend([
                "  â€¢ å¢åŠ  learning_rateï¼ˆå½“å‰å€¼ï¼š{}ï¼‰åŠ é€Ÿå­¦ä¹ ".format(self.learning_rate),
                "  â€¢ å¢åŠ  n_estimatorsï¼ˆå½“å‰å€¼ï¼š{}ï¼‰å»¶é•¿è®­ç»ƒ".format(self.n_estimators)
            ])

        if not overfitting_targets and not underfitting_targets:
            suggestions.extend([
                "âœ… æ¨¡å‹æ‹ŸåˆçŠ¶æ€è‰¯å¥½ï¼Œå½“å‰å‚æ•°é…ç½®åˆç†",
                f"ğŸ’¡ å¯å°è¯•å¾®è°ƒ learning_rate æˆ– {self.algorithm} ç‰¹å®šå‚æ•°è¿›ä¸€æ­¥ä¼˜åŒ–æ€§èƒ½"
            ])

        # æ·»åŠ é€šç”¨å»ºè®®
        suggestions.extend([
            f"ğŸ”§ å‚æ•°è°ƒæ•´ä½ç½®ï¼š{config_filename} -> [Model] section",
            f"ğŸ¯ å½“å‰ä½¿ç”¨ç®—æ³•ï¼š{self.algorithm}",
            "ğŸ’¾ ä¿®æ”¹é…ç½®åé‡å¯æœåŠ¡ç”Ÿæ•ˆ"
        ])

        return suggestions

    def get_last_training_diagnostics(self):
        """
        æ–°å¢æ–¹æ³•ï¼šè·å–æœ€åä¸€æ¬¡è®­ç»ƒçš„è¯Šæ–­ä¿¡æ¯
        ä¿æŒå‘åå…¼å®¹
        """
        return self.last_training_details

    def create_preprocessor(self, X, base_categorical):
        """
        åˆ›å»ºç‰¹å¾é¢„å¤„ç†å™¨

        :param X: ç‰¹å¾æ•°æ®
        :param base_categorical: åˆ†ç±»ç‰¹å¾åˆ—è¡¨
        :return: é¢„å¤„ç†å™¨å’Œç‰¹å¾é¡ºåº
        """
        numeric_cols = [col for col in X.columns if col not in base_categorical]
        categorical_cols = [col for col in X.columns if col in base_categorical]

        preprocessor = ColumnTransformer([
            ('num', StandardScaler(), numeric_cols),
            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
        ])

        X_proc = preprocessor.fit_transform(X)
        logger.debug(f"ç‰¹å¾é¢„å¤„ç†å®Œæˆï¼šæ•°å€¼ç‰¹å¾ {len(numeric_cols)} ä¸ªï¼Œåˆ†ç±»ç‰¹å¾ {len(categorical_cols)} ä¸ª")

        return preprocessor, X_proc, X.columns.tolist()

    def transform_features(self, preprocessor, X, fitted_feature_order):
        """
        è½¬æ¢ç‰¹å¾æ•°æ®

        :param preprocessor: é¢„å¤„ç†å™¨
        :param X: ç‰¹å¾æ•°æ®
        :param fitted_feature_order: è®­ç»ƒæ—¶çš„ç‰¹å¾é¡ºåº
        :return: è½¬æ¢åçš„ç‰¹å¾æ•°æ®
        """
        if list(X.columns) != fitted_feature_order:
            logger.warning(f"ç‰¹å¾é¡ºåºä¸è®­ç»ƒä¸ä¸€è‡´ï¼Œé‡æ–°æ’åº")
            missing_cols = set(fitted_feature_order) - set(X.columns)
            if missing_cols:
                raise ValueError(f"æ•°æ®ç¼ºå°‘ç‰¹å¾ï¼š{missing_cols}")
            X = X[fitted_feature_order]

        return preprocessor.transform(X)