"""
ç…¤çŸ¿ç“¦æ–¯é£é™©é¢„æµ‹ç³»ç»Ÿ - ä¸»æ¨¡å‹ç±»
æ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œæä¾›ç»Ÿä¸€çš„æ¨¡å‹æ¥å£
"""
import os
from datetime import datetime

import numpy as np
import pandas as pd
from filelock import FileLock
from loguru import logger
import configparser

# å¯¼å…¥å„åŠŸèƒ½æ¨¡å—
from config_utils import ConfigUtils
from fault_calculator import FaultCalculator
from regional_measure_calculator import RegionalMeasureCalculator
from data_preprocessor import DataPreprocessor
from model_trainer import ModelTrainer
from model_evaluator import ModelEvaluator
from model_predictor import ModelPredictor
from model_manager import ModelManager
from db_utils import DBUtils


class CoalMineRiskModel:
    """
    ç…¤çŸ¿ç“¦æ–¯é£é™©é¢„æµ‹æ¨¡å‹ï¼ˆLightGBMå¤šç›®æ ‡å›å½’ï¼‰
    ä½¿ç”¨ç»„åˆæ¨¡å¼è€Œéç»§æ‰¿æ¥æ•´åˆå„åŠŸèƒ½æ¨¡å—
    """

    def __init__(self, config_path="config.ini",
                 fault_calculator=None,
                 regional_calculator=None,
                 data_preprocessor=None,
                 model_trainer=None,
                 model_evaluator=None,
                 model_predictor=None,
                 model_manager=None,
                 db_utils=None):
        """
        æ¨¡å‹åˆå§‹åŒ–å…¥å£ï¼šè¯»å–é…ç½®â†’åˆå§‹åŒ–ç›®å½•â†’åŠ è½½å‚æ•°â†’åˆå§‹åŒ–ä¾èµ–â†’åŠ è½½å·²æœ‰æ¨¡å‹
        """
        # Step 1: åˆå§‹åŒ–é…ç½®
        self.config_path = config_path
        self.config = configparser.ConfigParser()
        self.config.read(config_path, encoding="utf-8")
        logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶ï¼š{config_path}")

        # Step 2: åˆå§‹åŒ–å„åŠŸèƒ½æ¨¡å—
        self.config_utils = ConfigUtils(config_path)
        self.fault_calculator = fault_calculator or FaultCalculator(config_path)
        self.regional_calculator = regional_calculator or RegionalMeasureCalculator(config_path)
        self.data_preprocessor = data_preprocessor or DataPreprocessor(config_path)
        self.model_trainer = model_trainer or ModelTrainer(self.config)
        self.model_trainer.config_filename = os.path.basename(config_path)
        self.model_evaluator = model_evaluator or ModelEvaluator(config_path)
        self.model_predictor = model_predictor or ModelPredictor()

        # Step 3: åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
        self.model_dir = self.config_utils._get_config_value("Model", "model_dir", "models")
        self.model_manager = model_manager or ModelManager(self.model_dir)

        # Step 4: åŠ è½½è®­ç»ƒæ ¸å¿ƒå‚æ•°
        self.full_train_threshold = self.config_utils._get_config_value("Model", "full_train_threshold", 6, is_int=True)
        self.min_train_samples = self.config_utils._get_config_value("Model", "min_train_samples", 6, is_int=True)

        # Step 5: æ¨¡å‹æ ¸å¿ƒçŠ¶æ€åˆå§‹åŒ–
        self.models = {}
        self.preprocessor = None
        self.training_features = None
        self.is_trained = False
        # ç®—æ³•é”å®šï¼šä¸€æ—¦å­˜åœ¨å·²è®­ç»ƒæ¨¡å‹ï¼ˆalgorithm.pklï¼‰ï¼Œç³»ç»Ÿè¿è¡Œä¸­å°†å¿½ç•¥é…ç½®æ–‡ä»¶é‡Œçš„algorithmå˜æ›´
        self.locked_algorithm = None
        self.total_samples = 0
        self.eval_history = []
        self.training_stats = []
        self.baseline_rmse = None
        self._fitted_feature_order = None

        # æ³¨æ„ï¼šä¸å†åŒ…å«ç“¦æ–¯æ¶Œå‡ºé‡ç›¸å…³çŠ¶æ€
        self.note = "æ¨¡å‹å·²é‡æ„ï¼Œç“¦æ–¯æ¶Œå‡ºé‡è¯·ä½¿ç”¨ç‹¬ç«‹çš„/calculate_gas_emission_sourceæ¥å£è®¡ç®—"
        # Step 6: åˆå§‹åŒ–æ•°æ®åº“å·¥å…·ä¸è·¨è¿›ç¨‹é”
        self.db = DBUtils(config_path=config_path)
        self.file_lock = FileLock(self.model_manager.lock_file_path)
        logger.info(f"è·¨è¿›ç¨‹é”åˆå§‹åŒ–å®Œæˆï¼Œé”æ–‡ä»¶è·¯å¾„ï¼š{self.model_manager.lock_file_path}")
        # Step 7: åŠ è½½å·²æœ‰æ¨¡å‹ä¸åŒæ­¥æ•°æ®åº“æ ·æœ¬æ•°
        self._load_model()
        # Step 7.1: ç®—æ³•é”å®šï¼ˆè‹¥å·²æœ‰æ¨¡å‹ï¼Œåç»­reload_config/retrainä¸å¾—åˆ‡æ¢ç®—æ³•ï¼‰
        locked = self.model_manager.get_locked_algorithm()
        if locked:
            self.locked_algorithm = locked
            # è¦†ç›–trainerç®—æ³•ï¼ˆå³ä½¿é…ç½®æ–‡ä»¶è¢«ä¿®æ”¹ï¼‰
            if getattr(self.model_trainer, "algorithm", None) != locked:
                logger.warning(
                    f"æ£€æµ‹åˆ°å·²è®­ç»ƒæ¨¡å‹ç®—æ³•é”å®šä¸º {locked}ï¼Œå°†è¦†ç›–å½“å‰é…ç½®/Trainerç®—æ³•ï¼ˆå¯åŠ¨åä¸å¯åˆ‡æ¢ï¼‰"
                )
            self.model_trainer.algorithm = locked
        else:
            # å°šæœªè®­ç»ƒè¿‡æ¨¡å‹ï¼šä»¥å½“å‰é…ç½®ä¸ºå‡†ï¼Œä½†è®°å½•ä¸ºâ€œé¢„æœŸé”å®šç®—æ³•â€
            self.locked_algorithm = getattr(self.model_trainer, "algorithm", None)

        try:
            self.total_samples = self.model_manager.get_total_samples_from_db(self.db)
        except Exception as e:
            self.total_samples = 0
            logger.warning(f"åŒæ­¥æ•°æ®åº“æ ·æœ¬æ•°å¤±è´¥ï¼š{str(e)}ï¼Œåˆå§‹åŒ–ä¸º0")

        # Step 8: æ§åˆ¶å°è¾“å‡ºåˆå§‹åŒ–ç»“æœ
        self._print_header("æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        self.current_config = config_path  # è®°å½•å½“å‰ä½¿ç”¨çš„é…ç½®æ–‡ä»¶
        logger.info(f"å½“å‰é…ç½®æ–‡ä»¶ï¼š{self.current_config}")
        # æ·»åŠ ç¼ºå¤±çš„å±æ€§åˆå§‹åŒ–
        self.fixed_evaluation_set = None  # å›ºå®šè¯„ä¼°æ•°æ®é›†
        if self.config.getboolean("Logging", "verbose_console", fallback=True):
            # ç¡®ä¿è¿™äº›å±æ€§å­˜åœ¨
            base_categorical_count = len(getattr(self.data_preprocessor, 'base_categorical', []))
            base_numeric_count = len(getattr(self.data_preprocessor, 'base_numeric', []))
            target_features_count = len(getattr(self.data_preprocessor, 'target_features', []))
            algorithm = getattr(self.model_trainer, 'algorithm', 'lightgbm')
            print(f"â”œâ”€ åˆ†ç±»ç‰¹å¾æ•°é‡ï¼š{base_categorical_count}")
            print(f"â”œâ”€ æ•°å€¼ç‰¹å¾æ•°é‡ï¼š{base_numeric_count}")
            print(f"â”œâ”€ é¢„æµ‹ç›®æ ‡æ•°é‡ï¼š{target_features_count}")
            print(f"â”œâ”€ ç´¯è®¡æ ·æœ¬æ•°ï¼š{self.total_samples}")
            print(f"â”œâ”€ ä½¿ç”¨ç®—æ³•ï¼š{algorithm}")
            print(f"â””â”€ æ¨¡å‹çŠ¶æ€ï¼š{'å·²è®­ç»ƒ' if self.is_trained else 'æœªè®­ç»ƒ'}")
            print("=" * 60)

    def reload_config(self, new_config_path=None, reload_database=False):
        """
        åŠ¨æ€é‡è½½é…ç½®ï¼ˆä¸é‡å¯æœåŠ¡ï¼‰
        ä¿æŒåŸæœ‰æ¨¡å‹çŠ¶æ€ï¼Œåªæ›´æ–°é…ç½®å‚æ•°

        :param new_config_path: strï¼Œæ–°é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤Noneï¼ˆä½¿ç”¨å½“å‰è·¯å¾„ï¼‰
        :param reload_database: boolï¼Œæ˜¯å¦é‡è½½æ•°æ®åº“é…ç½®ï¼Œé»˜è®¤Falseï¼ˆé¿å…ä¸å¿…è¦çš„è¿æ¥é‡å»ºï¼‰
        :return: boolï¼Œé‡è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("å¼€å§‹åŠ¨æ€é‡è½½ç³»ç»Ÿé…ç½®")

            # Step 1: å¤‡ä»½å…³é”®çŠ¶æ€
            current_models = self.models.copy() if self.models else {}
            current_total_samples = self.total_samples
            current_training_stats = self.training_stats.copy() if hasattr(self, 'training_stats') else []
            current_eval_history = self.eval_history.copy() if hasattr(self, 'eval_history') else []
            current_baseline_rmse = self.baseline_rmse if hasattr(self, 'baseline_rmse') else None
            current_is_trained = self.is_trained
            current_preprocessor = self.preprocessor
            current_training_features = self.training_features
            current_fitted_feature_order = self._fitted_feature_order
            current_locked_algorithm = getattr(self, 'locked_algorithm', None)
            current_modules = {
                'fault_calculator': self.fault_calculator,
                'regional_calculator': self.regional_calculator,
                'data_preprocessor': self.data_preprocessor,
                'model_trainer': self.model_trainer,
                'model_evaluator': self.model_evaluator,
                'model_manager': self.model_manager,
                'db': self.db
            }
            # Step 2: æ›´æ–°é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæä¾›äº†æ–°è·¯å¾„ï¼‰
            if new_config_path:
                self.config_path = new_config_path
                logger.info(f"åˆ‡æ¢åˆ°æ–°é…ç½®æ–‡ä»¶: {new_config_path}")

            # Step 3: é‡æ–°è¯»å–é…ç½®
            merged_config = configparser.ConfigParser()

            # é¦–å…ˆè¯»å–åŸºç¡€é…ç½® config.ini
            merged_config.read("config.ini", encoding="utf-8")
            logger.debug("åŸºç¡€é…ç½®æ–‡ä»¶ config.ini å·²åŠ è½½")

            # ç„¶åè¯»å–é˜¶æ®µé…ç½®ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå®ƒä¼šè¦†ç›–åŸºç¡€é…ç½®ä¸­çš„ç›¸åŒé¡¹
            if self.config_path and self.config_path != "config.ini":
                stage_config_read = merged_config.read(self.config_path, encoding="utf-8")
                if stage_config_read:
                    logger.debug(f"é˜¶æ®µé…ç½®æ–‡ä»¶ {self.config_path} å·²åŠ è½½å¹¶åˆå¹¶")
                else:
                    logger.warning(f"é˜¶æ®µé…ç½®æ–‡ä»¶ {self.config_path} è¯»å–å¤±è´¥ï¼Œä»…ä½¿ç”¨åŸºç¡€é…ç½®")

            # æ›´æ–°å½“å‰é…ç½®å¯¹è±¡
            self.config = merged_config
            logger.debug("é…ç½®æ–‡ä»¶åˆå¹¶å®Œæˆ")

            # Step 3.1: ç®—æ³•é”å®šï¼ˆå­˜åœ¨å·²è®­ç»ƒæ¨¡å‹æ—¶ï¼Œå¿½ç•¥é…ç½®æ–‡ä»¶ä¸­çš„algorithmå˜æ›´ï¼‰
            locked = current_locked_algorithm or self.model_manager.get_locked_algorithm()
            if locked:
            # è¯»å–ç”¨æˆ·é…ç½®ä¸­çš„algorithmï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰
                cfg_algo = merged_config.get("Model", "algorithm", fallback=locked).strip().lower()
                if cfg_algo != locked:
                   logger.warning(
                        f"é…ç½®æ–‡ä»¶algorithm={cfg_algo}å°†è¢«å¿½ç•¥ï¼›ç³»ç»Ÿå·²é”å®šç®—æ³•={locked}ï¼ˆå¯åŠ¨åä¸å¯åˆ‡æ¢ï¼‰"
                   )
                # å¼ºåˆ¶å†™å›åˆå¹¶åçš„é…ç½®ï¼Œç¡®ä¿åç»­é‡å»ºTrainerä½¿ç”¨é”å®šç®—æ³•
                if not merged_config.has_section("Model"):
                    merged_config.add_section("Model")
                merged_config.set("Model", "algorithm", locked)
                self.locked_algorithm = locked
            else:
                # æœªè®­ç»ƒæ¨¡å‹æ—¶ï¼Œå…è®¸ä»¥é…ç½®ä¸ºå‡†ï¼Œå¹¶è®°å½•ä¸ºé¢„æœŸé”å®šç®—æ³•
                self.locked_algorithm = merged_config.get("Model", "algorithm",fallback="lightgbm").strip().lower()

            # Step 4: é‡æ–°åˆå§‹åŒ–é…ç½®å·¥å…·å’Œå„æ¨¡å—,ä½¿ç”¨åˆå¹¶åçš„é…ç½®å¯¹è±¡æ¥åˆå§‹åŒ–å„æ¨¡å—
            self.config_utils = ConfigUtils(self.config_path)
            # é‡æ–°åˆå§‹åŒ–å„åŠŸèƒ½æ¨¡å—
            self.fault_calculator = FaultCalculator(self.config_path)
            self.regional_calculator = RegionalMeasureCalculator(self.config_path)
            self.data_preprocessor = DataPreprocessor(self.config_path)
            self.model_trainer = ModelTrainer(self.config)
            self.model_evaluator = ModelEvaluator(self.config_path)
            self.model_manager = ModelManager(self.model_dir)
            # ------------------------------------------------------------------
            # å¢é‡è®­ç»ƒçª—å£å‚æ•°ï¼ˆç”¨äºå•å¤©æ‰¹æ¬¡è‡ªåŠ¨è¡¥å†å²çª—å£ï¼Œé¿å…æ—¶åºæ¼‚ç§»ï¼‰
            # ------------------------------------------------------------------
            try:
                self.incremental_lookback_days = self.config.getint(
                    "Model", "incremental_lookback_days", fallback=7
                )
            except Exception:
                self.incremental_lookback_days = 7

            try:
                self.incremental_window_limit = self.config.getint(
                    "Model", "incremental_window_limit", fallback=3000
                )
            except Exception:
                self.incremental_window_limit = 3000

            logger.info(
                f"å¢é‡è®­ç»ƒçª—å£å‚æ•°åŠ è½½å®Œæˆï¼š"
                f"lookback_days={self.incremental_lookback_days}, "
                f"window_limit={self.incremental_window_limit}"
            )

            # Step 5: æ¡ä»¶æ€§é‡è½½æ•°æ®åº“é…ç½®
            if reload_database:
                logger.info("é‡è½½æ•°æ®åº“é…ç½®ï¼ˆå°†é‡å»ºæ•°æ®åº“è¿æ¥ï¼‰")
                db_reload_success = self.db.reload_config(self.config_path)
                if not db_reload_success:
                    logger.error("æ•°æ®åº“é…ç½®é‡è½½å¤±è´¥ï¼Œä½†ç»§ç»­å…¶ä»–é…ç½®é‡è½½")
            else:
                logger.info("è·³è¿‡æ•°æ®åº“é…ç½®é‡è½½ï¼ˆä½¿ç”¨ç°æœ‰è¿æ¥ï¼‰")
            # Step 6: é‡æ–°åŠ è½½æ¨¡å‹æ ¸å¿ƒå‚æ•°
            self.full_train_threshold = self.config_utils._get_config_value("Model", "full_train_threshold", 6,
                                                                            is_int=True)
            self.min_train_samples = self.config_utils._get_config_value("Model", "min_train_samples", 6,
                                                                         is_int=True)
            # Step 7: æ¢å¤å…³é”®çŠ¶æ€
            self.models = current_models
            self.total_samples = current_total_samples
            self.training_stats = current_training_stats
            self.eval_history = current_eval_history
            self.baseline_rmse = current_baseline_rmse
            self.is_trained = current_is_trained
            self.preprocessor = current_preprocessor
            self.training_features = current_training_features
            self._fitted_feature_order = current_fitted_feature_order

            # Step 8: åŒæ­¥é¢„å¤„ç†å™¨çŠ¶æ€
            if hasattr(self.data_preprocessor, 'is_trained'):
                self.data_preprocessor.is_trained = current_is_trained
            if hasattr(self.data_preprocessor, 'training_features') and self.training_features:
                self.data_preprocessor.training_features = self.training_features
            if hasattr(self, 'model_trainer'):
                self.model_trainer.config_filename = os.path.basename(self.config_path)
            logger.info("é…ç½®åŠ¨æ€é‡è½½å®Œæˆï¼Œæ‰€æœ‰æ¨¡å—å·²æ›´æ–°")
            self._print_result("é…ç½®åŠ¨æ€é‡è½½æˆåŠŸ")

            # è¾“å‡ºæ–°çš„å…³é”®å‚æ•°å€¼
            if self.config.getboolean("Logging", "verbose_console", fallback=True):
                print(f"â”œâ”€ æ–°çš„å…¨é‡è®­ç»ƒé˜ˆå€¼: {self.full_train_threshold}")
                print(f"â”œâ”€ æ–°çš„æœ€å°æ ·æœ¬æ•°: {self.min_train_samples}")
                print(f"â”œâ”€ æ–°çš„æ ‘æ•°é‡: {self.model_trainer.n_estimators}")
                print(f"â”œâ”€ æ–°çš„å­¦ä¹ ç‡: {self.model_trainer.learning_rate}")
                if reload_database:
                    print(f"â””â”€ æ•°æ®åº“é…ç½®å·²é‡è½½")
                else:
                    print(f"â””â”€ æ•°æ®åº“é…ç½®æœªé‡è½½ï¼ˆä½¿ç”¨ç°æœ‰è¿æ¥ï¼‰")

            return True


        except Exception as e:
            logger.error(f"é…ç½®åŠ¨æ€é‡è½½å¤±è´¥ï¼Œæ­£åœ¨æ¢å¤çŠ¶æ€ï¼š{str(e)}", exc_info=True)
            # æ¢å¤æ¨¡å—çŠ¶æ€
            self.fault_calculator = current_modules['fault_calculator']
            self.regional_calculator = current_modules['regional_calculator']
            self.data_preprocessor = current_modules['data_preprocessor']
            self.model_trainer = current_modules['model_trainer']
            self.model_evaluator = current_modules['model_evaluator']
            self.model_manager = current_modules['model_manager']
            self.db = current_modules['db']
            # æ¢å¤ç®—æ³•é”å®šçŠ¶æ€
            self.locked_algorithm = current_locked_algorithm
            self._print_result(f"é…ç½®é‡è½½å¤±è´¥ï¼š{str(e)}")
            return False

    def _print_header(self, title):
        """å§”æ‰˜ç»™config_utils"""
        self.config_utils._print_header(title)

    def _print_step(self, msg):
        """å§”æ‰˜ç»™config_utils"""
        self.config_utils._print_step(msg)

    def _print_result(self, msg):
        """å§”æ‰˜ç»™config_utils"""
        self.config_utils._print_result(msg)

    def _load_model(self):
        """åŠ è½½å·²æœ‰æ¨¡å‹ç»„ä»¶"""
        self._print_header("åŠ è½½å·²æœ‰æ¨¡å‹")
        try:
            # ç¡®ä¿target_featureså±æ€§å­˜åœ¨
            target_features = getattr(self.data_preprocessor, 'target_features', [])
            if not target_features:
                logger.warning("ç›®æ ‡ç‰¹å¾åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•åŠ è½½æ¨¡å‹")
                self.is_trained = False
                return

            (self.preprocessor, self.training_features,
             self.models, self.is_trained) = self.model_manager.load_model(target_features)

            if self.is_trained:
                self.data_preprocessor.is_trained = True
                self.data_preprocessor.training_features = self.training_features
                self._print_step("æ¨¡å‹åŠ è½½æˆåŠŸ")
            else:
                self._print_step("æ¨¡å‹åŠ è½½å¤±è´¥æˆ–æœªè®­ç»ƒï¼Œéœ€ä»å¤´è®­ç»ƒ")
        except Exception as e:
            self.is_trained = False
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥ï¼š{str(e)}", exc_info=True)
            self._print_step(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}ï¼Œéœ€ä»å¤´è®­ç»ƒ")
        finally:
            if self.config.getboolean("Logging", "verbose_console", fallback=True):
                print("-" * 60)

    def calculate_fault_influence_strength(self, data):
        """è®¡ç®—æ–­å±‚å½±å“ç³»æ•°ï¼ˆå§”æ‰˜ç»™FaultCalculatorï¼‰"""
        return self.fault_calculator.calculate_fault_influence_strength(data, self.db)

    def calculate_regional_measure_strength(self, measures):
        """è®¡ç®—åŒºåŸŸæªæ–½å¼ºåº¦ï¼ˆå§”æ‰˜ç»™RegionalMeasureCalculatorï¼‰"""
        return self.regional_calculator.calculate_regional_measure_strength(measures)

    def _preprocess_data(self, data, is_training=True):
        """æ•°æ®é¢„å¤„ç†ï¼ˆå§”æ‰˜ç»™DataPreprocessorï¼‰"""
        if is_training:
            df, training_features = self.data_preprocessor.preprocess_data(
                data, is_training, self.fault_calculator, self.db
            )
            self.training_features = training_features
            return df
        else:
            return self.data_preprocessor.preprocess_data(
                data, is_training, self.fault_calculator, self.db
            )

    def _print_training_diagnosis(self):
        """
        æ–°å¢ï¼šæ‰“å°è®­ç»ƒè¯Šæ–­ç»“æœå’Œå‚æ•°å»ºè®®
        ä¸ä¿®æ”¹åŸæœ‰è®­ç»ƒæµç¨‹
        """
        training_details = self.model_trainer.get_last_training_diagnostics()
        if not training_details:
            return

        target_performance = training_details.get('target_performance', {})
        suggestions = training_details.get('parameter_suggestions', [])

        self._print_header("æ¨¡å‹è®­ç»ƒè¯Šæ–­ç»“æœ")

        # æ‰“å°å„ç›®æ ‡æ€§èƒ½
        for target, perf in target_performance.items():
            status = "âœ… è‰¯å¥½"
            if perf.get('is_overfitting', False):
                status = "âš ï¸ è¿‡æ‹Ÿåˆ"
            elif perf.get('is_underfitting', False):
                status = "ğŸ“‰ æ¬ æ‹Ÿåˆ"

            validation_note = "(éªŒè¯é›†)" if perf.get('use_validation', False) else "(è®­ç»ƒé›†)"
            self._print_step(
                f"{target}: è®­ç»ƒRMSE={perf['train_rmse']}, "
                f"{validation_note}RMSE={perf['val_rmse']}, "
                f"è¿‡æ‹Ÿåˆæ¯”ç‡={perf['overfitting_ratio']} {status}"
            )

        # æ‰“å°å‚æ•°å»ºè®®
        if suggestions:
            self._print_header("å‚æ•°è°ƒæ•´å»ºè®®")
            for suggestion in suggestions:
                self._print_step(suggestion)

    def train(self, data, epochs=1):
        """
        å…¬å¼€æ–¹æ³•ï¼šæ¨¡å‹è®­ç»ƒæ¥å£
        """
        with self.file_lock:
            self._print_header("æ¨¡å‹è®­ç»ƒå¼€å§‹")
            train_start = datetime.now()
            initial_samples = self.total_samples
            db_conn = None
            db_trans = None
            custom_create_time = None
            saved_count = 0

            try:
                # Step 1: æ•°æ®é¢„å¤„ç† - æ·»åŠ è¯¦ç»†æ—¥å¿—å’Œé”™è¯¯å¤„ç†
                logger.info(f"å¼€å§‹æ•°æ®é¢„å¤„ç†ï¼Œè¾“å…¥æ•°æ®æ ·æœ¬æ•°: {len(data) if isinstance(data, list) else 'unknown'}")

                try:
                    df = self._preprocess_data(data, is_training=True)
                    logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼ŒDataFrameå½¢çŠ¶: {df.shape}")
                except Exception as e:
                    logger.error(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
                    raise ValueError(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}")

                # æ£€æŸ¥å…³é”®åˆ—æ˜¯å¦å­˜åœ¨
                logger.info(f"DataFrameåˆ—å: {list(df.columns)}")
                logger.info(f"è®­ç»ƒç‰¹å¾: {self.training_features}")
                logger.info(f"ç›®æ ‡ç‰¹å¾: {self.data_preprocessor.target_features}")

                # ============ å…ˆæ‰§è¡Œï¼šè‡ªåŠ¨é…ç½®åˆ‡æ¢é€»è¾‘ï¼ˆç¡®ä¿è®­ç»ƒç‰¹å¾ä»¥æœ€ç»ˆé…ç½®ä¸ºå‡†ï¼‰ ============
                # åˆ¤æ–­æ˜¯å¦åˆæ¬¡è®­ç»ƒï¼ˆæ¨¡å‹æœªè®­ç»ƒä¸”æ•°æ®åº“æ ·æœ¬æ•°ä¸º0ï¼‰
                is_initial_training = not self.is_trained and initial_samples == 0

                # åˆ¤æ–­æ•°æ®é‡å¤§å°ï¼ˆä½¿ç”¨å…¨é‡è®­ç»ƒé˜ˆå€¼ä½œä¸ºåˆ¤æ–­æ ‡å‡†ï¼‰
                is_large_data = len(df) >= self.full_train_threshold

                # è‡ªåŠ¨é…ç½®åˆ‡æ¢å†³ç­–
                if is_initial_training and is_large_data:
                    # åˆæ¬¡å¤§é‡æ•°æ®è®­ç»ƒ â†’ ä½¿ç”¨ phase1 é…ç½®
                    target_config = "config_phase1.ini"
                    reason = "åˆæ¬¡å¤§é‡æ•°æ®è®­ç»ƒ"
                elif self.is_trained and not is_large_data:
                    # åç»­å°‘é‡æ•°æ®å¢é‡è®­ç»ƒ â†’ ä½¿ç”¨ phase2 é…ç½®
                    target_config = "config_phase2.ini"
                    reason = "å°‘é‡æ•°æ®å¢é‡è®­ç»ƒ"
                else:
                    # å…¶ä»–æƒ…å†µä¿æŒå½“å‰é…ç½®
                    target_config = None
                    reason = "ä¿æŒå½“å‰é…ç½®"

                # æ‰§è¡Œé…ç½®åˆ‡æ¢ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if target_config and getattr(self, 'current_config', None) != target_config:
                    logger.info(f"è‡ªåŠ¨é…ç½®åˆ‡æ¢ï¼š{reason} â†’ {target_config}")
                    success = self.reload_config(target_config)
                    if success:
                        self.current_config = target_config
                        self.model_trainer.config_filename = os.path.basename(target_config)
                        self._print_step(f"âœ… é…ç½®å·²åˆ‡æ¢ï¼š{target_config}ï¼ˆ{reason}ï¼‰")
                    else:
                        logger.error(f"é…ç½®åˆ‡æ¢å¤±è´¥ï¼š{target_config}")
                        self._print_step(f"âŒ é…ç½®åˆ‡æ¢å¤±è´¥ï¼š{target_config}")
                elif target_config:
                    logger.debug(f"é…ç½®å·²æ˜¯æœ€æ–°ï¼š{target_config}")
                else:
                    logger.debug(f"æ— éœ€åˆ‡æ¢é…ç½®ï¼š{reason}")
                # ============ è‡ªåŠ¨é…ç½®åˆ‡æ¢é€»è¾‘ç»“æŸ ============

                # é…ç½®åˆ‡æ¢åå†æ‰“å°ä¸€æ¬¡ï¼Œä¾¿äºå®¡è®¡æœ€ç»ˆç”Ÿæ•ˆçš„ç‰¹å¾é…ç½®
                logger.info(f"[é…ç½®åˆ‡æ¢å] è®­ç»ƒç‰¹å¾: {self.training_features}")
                # ============ å¼ºåˆ¶å‰”é™¤ gas_emission_q ç›¸å…³ç‰¹å¾ï¼ˆæ ¹æ²»ç¼ºåˆ—å¯¼è‡´è®­ç»ƒå¤±è´¥ï¼‰ ============
                try:
                    if self.training_features:
                        before = list(self.training_features)
                        self.training_features = [
                            f for f in self.training_features
                            if not (f == "gas_emission_q"
                                    or f.startswith("gas_emission_q_")
                                    or "gas_emission_q_" in f)
                        ]
                        removed = [f for f in before if f not in self.training_features]
                        if removed:
                            logger.warning(f"å·²å¼ºåˆ¶å‰”é™¤ {len(removed)} ä¸ª gas_emission_q ç›¸å…³ç‰¹å¾ï¼š{removed}")
                            logger.info(f"å‰”é™¤åè®­ç»ƒç‰¹å¾æ•°ï¼š{len(before)} -> {len(self.training_features)}")
                            logger.info(f"å‰”é™¤åè®­ç»ƒç‰¹å¾åˆ—è¡¨ï¼š{self.training_features}")

                        # åŒæ­¥ç»™ data_preprocessorï¼ˆä¿è¯è®­ç»ƒ/é¢„æµ‹/è¯„ä¼°ä¸€è‡´ï¼‰
                        if hasattr(self.data_preprocessor, "training_features"):
                            self.data_preprocessor.training_features = self.training_features
                except Exception as _e:
                    logger.warning(f"å¼ºåˆ¶å‰”é™¤ gas_emission_q ç‰¹å¾å¤±è´¥ï¼ˆå·²å¿½ç•¥ï¼‰ï¼š{repr(_e)}", exc_info=True)
                # ============ å¼ºåˆ¶å‰”é™¤ç»“æŸ ============

                # ============ æ–°å¢ï¼šè‡ªåŠ¨ç‰¹å¾é™çº§ / è‡ªåŠ¨æ¢å¤ï¼ˆå‡çº§ï¼‰ ============
                # å†·å¯åŠ¨é˜¶æ®µå¸¸è§ï¼šdays_* / distance_time_interaction / advance_rate æ’ä¸º0æˆ–æ— ä¿¡æ¯é‡ï¼Œè‡ªåŠ¨å‰”é™¤é™å™ªï¼›
                # å½“åç»­æ•°æ®å…·å¤‡æ—¶é—´è·¨åº¦/æ¨è¿›ä¿¡æ¯æ—¶è‡ªåŠ¨æ¢å¤ã€‚
                try:
                    if not self.training_features:
                        raise ValueError("training_featuresä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œè‡ªåŠ¨ç‰¹å¾é™çº§")

                    # å›ºåŒ–â€œé…ç½®å±‚é¢â€çš„å…¨é‡ç‰¹å¾ï¼ˆé¦–æ¬¡è¿›å…¥trainæ—¶è®°å½•ä¸€æ¬¡ï¼‰
                    if not hasattr(self, "_configured_training_features") or not self._configured_training_features:
                        self._configured_training_features = list(self.training_features)

                    degrade_candidates = [
                        "days_since_start",
                        "days_in_workface",
                        "distance_time_interaction",
                        "advance_rate",
                    ]

                    def _is_degenerate_feature(_df: pd.DataFrame, col: str):
                        """åˆ¤æ–­ç‰¹å¾æ˜¯å¦é€€åŒ–ï¼›è¿”å›(æ˜¯å¦é€€åŒ–, åŸå› )"""
                        if col not in _df.columns:
                            return True, "ç¼ºåˆ—"
                        s = _df[col]
                        # ç»Ÿä¸€å¤„ç†inf
                        try:
                            if pd.api.types.is_numeric_dtype(s):
                                s = s.replace([np.inf, -np.inf], np.nan)
                        except Exception:
                            pass
                        # å…¨ç©º
                        if s.isna().all():
                            return True, "å…¨ç¼ºå¤±"
                        # å¸¸æ•°åˆ—ï¼ˆå«å…¨ä¸º0ï¼‰
                        try:
                            nunq = int(s.nunique(dropna=True))
                        except Exception:
                            nunq = 2  # ä¿å®ˆï¼šä¸åˆ¤é€€åŒ–
                        if nunq <= 1:
                            if pd.api.types.is_numeric_dtype(s):
                                try:
                                    if (s.fillna(0) == 0).all():
                                        return True, "å¸¸æ•°åˆ—ï¼ˆå…¨ä¸º0ï¼‰"
                                except Exception:
                                    pass
                            return True, "å¸¸æ•°åˆ—ï¼ˆnunique<=1ï¼‰"
                        return False, "OK"
                    # 1) è‡ªåŠ¨é™çº§ï¼šå‰”é™¤é€€åŒ–ç‰¹å¾
                    removed = []
                    reasons_map = {}
                    for c in degrade_candidates:
                        deg, why = _is_degenerate_feature(df, c)
                        if deg and c in self.training_features:
                            removed.append(c)
                            reasons_map[c] = why
                    # 2) è‡ªåŠ¨æ¢å¤ï¼šå½“é€€åŒ–ç‰¹å¾åœ¨æ–°æ•°æ®ä¸­æœ‰ä¿¡æ¯é‡åˆ™æ¢å¤ï¼ˆæŒ‰é…ç½®é¡ºåºï¼‰
                    restored = []
                    for c in degrade_candidates:
                        if c in getattr(self, "_configured_training_features", []) and c not in self.training_features:
                            deg, _ = _is_degenerate_feature(df, c)
                            if not deg:
                                restored.append(c)
                    if removed or restored:
                        before_cnt = len(self.training_features)
                        active = list(self.training_features)
                        # å…ˆæ¢å¤ï¼ˆæŒ‰é…ç½®é¡ºåºï¼‰
                        if restored:
                            cfg = list(self._configured_training_features)
                            active_set = set(active) | set(restored)
                            active = [x for x in cfg if x in active_set]
                        # å†å‰”é™¤
                        if removed:
                            active = [x for x in active if x not in set(removed)]
                        self.training_features = active
                        # åŒæ­¥åˆ°é¢„å¤„ç†å™¨ï¼ˆé¢„æµ‹/è¯„ä¼°å¯¹é½éœ€è¦ï¼‰
                        if hasattr(self.data_preprocessor, "training_features"):
                            self.data_preprocessor.training_features = self.training_features
                        after_cnt = len(self.training_features)
                        if removed:
                            logger.warning(
                                f"è‡ªåŠ¨ç‰¹å¾é™çº§ï¼šå‰”é™¤{len(removed)}ä¸ªé€€åŒ–ç‰¹å¾ -> {removed}ï¼ŒåŸå› ={reasons_map}"
                            )
                        if restored:
                            logger.info(f"è‡ªåŠ¨ç‰¹å¾æ¢å¤ï¼šæ¢å¤{len(restored)}ä¸ªç‰¹å¾ -> {restored}")
                        logger.info(f"æœ¬æ¬¡è®­ç»ƒç”Ÿæ•ˆç‰¹å¾æ•°ï¼š{before_cnt} -> {after_cnt}")
                        logger.info(f"æœ¬æ¬¡è®­ç»ƒç”Ÿæ•ˆç‰¹å¾åˆ—è¡¨ï¼š{self.training_features}")
                except Exception as _e:
                    logger.warning(f"è‡ªåŠ¨ç‰¹å¾é™çº§/æ¢å¤æ‰§è¡Œå¤±è´¥ï¼ˆå·²å¿½ç•¥ï¼‰ï¼š{repr(_e)}", exc_info=True)
                # ============ è‡ªåŠ¨ç‰¹å¾é™çº§ / è‡ªåŠ¨æ¢å¤ç»“æŸ ============

                if len(df) < self.min_train_samples:
                    msg = f"æ ·æœ¬æ•° {len(df)} < æœ€å°è®­ç»ƒæ ·æœ¬æ•° {self.min_train_samples}ï¼Œè·³è¿‡è®­ç»ƒ"
                    logger.warning(msg)
                    self._print_result(msg)
                    return {
                        "status": "warning",
                        "message": msg,
                        "training_stats": {"processed_samples": len(df), "training_performed": False}
                    }
                # å› ä¸ºå¢é‡è®­ç»ƒä¼šå›ælookbackçª—å£ï¼Œæœ€ç»ˆè®­ç»ƒé›†(train_df)å¯èƒ½é€šè¿‡DBè¡¥é½å¢å¼ºç‰¹å¾åˆ—
                missing_features = []
                if self.training_features:
                    missing_features = [f for f in self.training_features if f not in df.columns]
                if missing_features:
                    logger.warning(f"æœ¬æ‰¹è®­ç»ƒdfç¼ºå°‘ç‰¹å¾(å¯èƒ½ç”±çª—å£train_dfè¡¥é½)ï¼š{missing_features}")
                    logger.debug(f"æœ¬æ‰¹dfå¯ç”¨åˆ—: {list(df.columns)}")
                missing_targets = []
                if self.data_preprocessor.target_features:
                    missing_targets = [t for t in self.data_preprocessor.target_features if t not in df.columns]
                if missing_targets:
                    logger.error(f"è®­ç»ƒæ•°æ®ç¼ºå°‘ç›®æ ‡ç‰¹å¾: {missing_targets}")
                    raise ValueError(f"è®­ç»ƒæ•°æ®ç¼ºå°‘ç›®æ ‡ç‰¹å¾: {missing_targets}")
                # Step 2: å¼€å¯æ•°æ®åº“äº‹åŠ¡
                db_conn = self.db._get_connection()
                db_trans = db_conn.begin()
                logger.info("æ•°æ®åº“äº‹åŠ¡å·²å¼€å¯ï¼Œå‡†å¤‡ä¿å­˜è®­ç»ƒæ•°æ®")
                # Step 3: ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
                custom_create_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
                saved_count = self.db.save_training_data(
                    df=df,
                    conn=db_conn,
                    trans=db_trans,
                    custom_create_time=custom_create_time
                )
                if saved_count == 0:
                    raise ValueError("æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥ï¼ˆä¿å­˜æ¡æ•°ä¸º0ï¼‰")
                logger.info(f"äº‹åŠ¡ä¸­æ’å…¥ {saved_count} æ¡è®­ç»ƒæ•°æ®ï¼ˆæœªæäº¤ï¼‰")
                # Step 4: åˆ¤æ–­è®­ç»ƒæ¨¡å¼ï¼ˆå…ˆåˆ¤æ–­æ¨¡å¼ï¼Œå†å†³å®šæ˜¯å¦å…è®¸é‡å»ºé¢„å¤„ç†å™¨ï¼‰
                current_total = initial_samples + len(df)
                threshold_hit = (current_total % self.full_train_threshold == 0)
                # ä¼ é€’æ•°æ®é‡ä¿¡æ¯çš„æ€§èƒ½æ£€æŸ¥
                perf_trigger = self.model_evaluator._performance_trigger_check(
                    self.eval_history,
                    self.baseline_rmse,
                    len(df)  # ä¼ é€’å½“å‰è®­ç»ƒæ•°æ®é‡
                )
                if perf_trigger:
                    do_full_train = True
                    reason = "æ€§èƒ½ä¸‹é™è§¦å‘"
                elif threshold_hit:
                    do_full_train = True
                    reason = "æ ·æœ¬æ•°è¾¾é˜ˆå€¼è§¦å‘"
                elif not self.is_trained:
                    do_full_train = True
                    reason = "é¦–æ¬¡è®­ç»ƒ"
                else:
                    do_full_train = False
                    reason = "å¢é‡è®­ç»ƒ"
                self._print_step(f"è®­ç»ƒæ¨¡å¼åˆ¤æ–­ï¼š{reason}")
                # Step 5: æ„å»ºè®­ç»ƒé›†ï¼ˆå¢é‡è®­ç»ƒï¼šåªè¦è¿›å…¥å¢é‡æ¨¡å¼å°±å¼ºåˆ¶lookbackçª—å£ï¼Œç¡®ä¿å¿…ç„¶ç”Ÿæ•ˆï¼‰
                train_df = df
                if (not do_full_train) and self.is_trained:
                    # è¯»å–çª—å£å‚æ•°
                    try:
                        lookback_days = self.config.getint("Model", "incremental_lookback_days", fallback=14)
                    except Exception:
                        lookback_days = 14
                    try:
                        window_limit = self.config.getint("Model", "incremental_window_limit", fallback=2000)
                    except Exception:
                        window_limit = 2000
                    # ä»¥â€œæœ¬æ‰¹æ¬¡æœ€å¤§æ—¥æœŸâ€ä½œä¸ºçª—å£ä¸Šç•Œï¼ˆä¿è¯åŒäº‹åŠ¡æœªæäº¤æ•°æ®ä¹Ÿèƒ½çº³å…¥ï¼‰
                    max_date = None
                    try:
                        if "measurement_date" in df.columns:
                            _mx = pd.to_datetime(df["measurement_date"], errors="coerce").max()
                            if pd.notna(_mx):
                                max_date = str(_mx.date())
                    except Exception:
                        max_date = None
                    workface_ids = None
                    try:
                        if "workface_id" in df.columns:
                            workface_ids = sorted({int(float(x)) for x in df["workface_id"].dropna().unique()})
                    except Exception:
                        workface_ids = None
                    try:
                        # è¯´æ˜ï¼š
                        #  - è¿™é‡Œç”¨åŒä¸€ä¸ª db_connï¼ˆåŒäº‹åŠ¡ï¼‰å–æ•°ï¼Œå¯â€œçœ‹åˆ°â€åˆšæ’å…¥æœªæäº¤çš„æ•°æ®
                        #  - ä»è€Œä¿è¯çª—å£ä¸€å®šåŒ…å«å½“å‰æ‰¹æ¬¡ + å†å²lookback
                        train_df = self.db.fetch_recent_training_window_with_features(
                            workface_ids=workface_ids,
                            max_date=max_date,
                            lookback_days=int(lookback_days),
                            limit=int(window_limit),
                            conn=db_conn
                        )
                        if train_df is None or train_df.empty:
                            logger.warning("å¢é‡è®­ç»ƒçª—å£å–æ•°ä¸ºç©ºï¼Œå›é€€ä¸ºä»…æœ¬æ‰¹æ¬¡dfè®­ç»ƒ")
                            train_df = df
                        else:
                            logger.info(
                                f"å¢é‡è®­ç»ƒçª—å£å·²ç”Ÿæ•ˆï¼šlookback_days={lookback_days}ï¼Œwindow_limit={window_limit}ï¼Œ"
                                f"å®é™…è®­ç»ƒæ ·æœ¬={len(train_df)}ï¼ˆå«å½“å‰æ‰¹æ¬¡ï¼‰"
                            )
                    except Exception as _e:
                        logger.warning(f"å¢é‡è®­ç»ƒçª—å£æ‹‰å–å¤±è´¥ï¼ˆå›é€€ä¸ºä»…æœ¬æ‰¹æ¬¡dfè®­ç»ƒï¼‰ï¼š{repr(_e)}", exc_info=True)
                        train_df = df
                # è®­ç»ƒç›®æ ‡ y å¿…é¡»æ¥è‡ª train_dfï¼ˆå¦åˆ™çª—å£å³ä½¿æ‹¼äº†ä¹Ÿç­‰äºæ²¡ç”Ÿæ•ˆï¼‰
                y = train_df[self.data_preprocessor.target_features].values
                if (not do_full_train) and self.is_trained:
                    # --- å¢é‡è®­ç»ƒï¼šå¼ºåˆ¶ä½¿ç”¨å·²fitçš„ç‰¹å¾é¡ºåºä¸å·²fitçš„é¢„å¤„ç†å™¨ ---
                    if not getattr(self, "_fitted_feature_order", None):
                        logger.warning("å¢é‡è®­ç»ƒï¼šæœªæ‰¾åˆ°å·²fitç‰¹å¾é¡ºåºï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸ºå…¨é‡è®­ç»ƒ")
                        do_full_train = True
                        reason = "ç‰¹å¾é¡ºåºç¼ºå¤±è§¦å‘å…¨é‡è®­ç»ƒ"
                    if self.preprocessor is None:
                        logger.warning("å¢é‡è®­ç»ƒï¼šæœªæ‰¾åˆ°å·²fité¢„å¤„ç†å™¨ï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸ºå…¨é‡è®­ç»ƒ")
                        do_full_train = True
                        reason = "é¢„å¤„ç†å™¨ç¼ºå¤±è§¦å‘å…¨é‡è®­ç»ƒ"

                if do_full_train:
                    # å…¨é‡è®­ç»ƒï¼šå…è®¸ä½¿ç”¨å½“å‰training_featureså¹¶é‡æ–°fité¢„å¤„ç†å™¨
                    X = train_df[self.training_features]
                    self._fitted_feature_order = X.columns.tolist()
                    logger.info(f"æ„å»ºè®­ç»ƒé›†: Xå½¢çŠ¶={X.shape}, yå½¢çŠ¶={y.shape}")
                    self.preprocessor, X_proc, _ = self.model_trainer.create_preprocessor(
                        X, self.data_preprocessor.base_categorical
                    )
                else:
                    # å¢é‡è®­ç»ƒï¼šå›ºå®šç‰¹å¾é›†åˆï¼Œç¼ºå¤±åˆ—è¡¥0ï¼Œé¿å…â€œè‡ªåŠ¨ç‰¹å¾é™çº§/æ¢å¤â€å¯¼è‡´ç»´åº¦å˜åŒ–
                    fitted_cols = list(self._fitted_feature_order)
                    missing = [c for c in fitted_cols if c not in train_df.columns]
                    if missing:
                        # è¿™é‡Œä¸ç›´æ¥æŠ¥é”™ï¼Œè€Œæ˜¯è¡¥0ï¼šå› ä¸ºæœ‰äº›å¢å¼ºç‰¹å¾åœ¨æ–°æ‰¹æ¬¡/å†å²çª—å£å¯èƒ½æš‚æ—¶ä¸å¯å¾—
                        logger.warning(f"å¢é‡è®­ç»ƒï¼šè®­ç»ƒé›†(df, å«çª—å£)ç¼ºå¤±{len(missing)}ä¸ªå·²fitç‰¹å¾ï¼Œå°†è¡¥0ï¼š{missing}")
                    X = train_df.reindex(columns=fitted_cols, fill_value=0)
                    logger.info(f"æ„å»ºè®­ç»ƒé›†(å¢é‡): Xå½¢çŠ¶={X.shape}, yå½¢çŠ¶={y.shape}")
                    # å…³é”®ï¼šåªtransformï¼Œä¸å†fitï¼Œç¡®ä¿è¾“å‡ºç»´åº¦æ’å®š
                    X_proc = self.preprocessor.transform(X)
                # Step 6: æ‰§è¡Œè®­ç»ƒ
                if do_full_train:
                    self.models = self.model_trainer._full_train(X_proc, y, self.data_preprocessor.target_features)
                else:
                    self.models = self.model_trainer._incremental_train(
                        X_proc, y, self.data_preprocessor.target_features, self.models
                    )
                # é˜²å¾¡ï¼šè®­ç»ƒå™¨è‹¥å¼‚å¸¸è¿”å›édictï¼ˆå†å²é—ç•™ï¼‰ï¼Œç«‹å³å¤±è´¥è§¦å‘å›æ»šï¼Œé¿å…â€œæ•°æ®æäº¤ä½†æ¨¡å‹å¼‚å¸¸â€
                if not isinstance(self.models, dict):
                    raise ValueError(f"è®­ç»ƒå¤±è´¥ï¼šmodelsç±»å‹å¼‚å¸¸({type(self.models).__name__})ï¼Œå°†è§¦å‘å›æ»š")

                # Step 8: æäº¤äº‹åŠ¡+æ›´æ–°çŠ¶æ€
                db_trans.commit()
                logger.info(f"äº‹åŠ¡æäº¤æˆåŠŸï¼Œ{saved_count} æ¡æ•°æ®å·²æŒä¹…åŒ–")
                # å…³é”®ä¿®å¤ï¼šäº‹åŠ¡å·²æäº¤ï¼Œåç»­å³ä½¿è¯„ä¼°å¤±è´¥ä¹Ÿä¸å…è®¸å†rollback
                db_trans = None
                self.total_samples = self.model_manager.get_total_samples_from_db(self.db)
                new_samples = self.total_samples - initial_samples
                self._print_step(f"æ ·æœ¬æ•°æ›´æ–°ï¼šæ–°å¢ {new_samples} æ¡ï¼Œç´¯è®¡ {self.total_samples} æ¡")
                # Step 9: ä¿å­˜æ¨¡å‹+æ ‡è®°è®­ç»ƒçŠ¶æ€
                self.model_manager.save_model(
                    self.models, self.preprocessor, self.training_features, self.data_preprocessor.target_features
                )
                self.is_trained = True
                self.data_preprocessor.is_trained = True
                self.data_preprocessor.training_features = self.training_features
                # Step 10: è®­ç»ƒåè¯„ä¼°ï¼ˆè¯„ä¼°å¤±è´¥åªé™çº§ï¼Œä¸å½±å“è®­ç»ƒæˆåŠŸï¼‰
                if len(df) <= 5:
                    eval_result = {
                        "status": "skipped",
                        "message": f"æ•°æ®é‡è¿‡å°‘({len(df)}æ¡)ï¼Œè·³è¿‡è¯„ä¼°é¿å…è¯¯åˆ¤",
                        "avg_rmse": None
                    }
                    agg_rmse = None
                    logger.info(f"å°æ•°æ®è®­ç»ƒ({len(df)}æ¡)ï¼šè·³è¿‡æ€§èƒ½è¯„ä¼°")
                else:
                    try:
                        eval_result = self.evaluate_model()
                        agg_rmse = eval_result.get("avg_rmse")
                    except Exception as _e:
                        # è¯„ä¼°å¤±è´¥ä¸åº”å¯¼è‡´è®­ç»ƒå¤±è´¥ï¼Œæ›´ä¸å…è®¸å›æ»šå·²æäº¤æ•°æ®
                        logger.error(f"è®­ç»ƒåè¯„ä¼°å¤±è´¥ï¼ˆå°†é™çº§ä¸ºwarningï¼Œä¸å½±å“è®­ç»ƒæäº¤ï¼‰ï¼š{repr(_e)}", exc_info=True)
                        eval_result = {"status": "warning", "message": f"è¯„ä¼°å¤±è´¥ï¼š{str(_e)}"}
                        agg_rmse = None
                # Step 11: æ™ºèƒ½è®¾ç½®æ€§èƒ½åŸºçº¿
                if agg_rmse and self.baseline_rmse is None:
                    # é¦–æ¬¡è®¾ç½®åŸºçº¿
                    self.baseline_rmse = agg_rmse
                    logger.info(f"è®¾ç½®åˆå§‹æ€§èƒ½åŸºçº¿: {agg_rmse:.4f}")
                elif agg_rmse and len(df) >= 10:
                    # åªæœ‰æ•°æ®é‡è¶³å¤Ÿæ—¶æ‰æ›´æ–°åŸºçº¿ï¼Œé¿å…å°æ•°æ®å¹²æ‰°
                    self.baseline_rmse = agg_rmse
                    logger.info(f"æ›´æ–°æ€§èƒ½åŸºçº¿: {agg_rmse:.4f}")
                # Step 12: æ€§èƒ½å›æ»šæ£€æŸ¥ï¼ˆæ·»åŠ å°æ•°æ®ä¿æŠ¤ï¼‰
                # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¡®çš„å˜é‡å agg_rmse è€Œä¸æ˜¯ current_rmse
                if (eval_result["status"] == "success" and
                        agg_rmse is not None and  # ä¿®æ­£ï¼šä½¿ç”¨ agg_rmse
                        self.baseline_rmse is not None and
                        len(df) > 10):  # åªæœ‰æ•°æ®é‡>10æ—¶æ‰æ£€æŸ¥æ€§èƒ½ä¸‹é™
                    drop_ratio = (agg_rmse - self.baseline_rmse) / self.baseline_rmse  # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¡®çš„å˜é‡
                    # æ·»åŠ è¯¦ç»†çš„æ€§èƒ½è¯Šæ–­æ—¥å¿—
                    logger.info(f"=== æ€§èƒ½æ£€æŸ¥è¯Šæ–­ ===")
                    logger.info(f"è®­ç»ƒæ•°æ®: {len(df)}æ¡, å½“å‰RMSE: {agg_rmse:.4f}")  # ä¿®æ­£ï¼šä½¿ç”¨ agg_rmse
                    logger.info(f"åŸºçº¿RMSE: {self.baseline_rmse:.4f}, ä¸‹é™æ¯”ä¾‹: {drop_ratio:.2%}")
                    logger.info(f"é˜ˆå€¼: {self.model_evaluator.perf_drop_ratio * 2:.2%}")
                    if drop_ratio > self.model_evaluator.perf_drop_ratio * 2:
                        logger.warning(f"æ€§èƒ½ä¸‹é™è¿‡å¤šï¼ˆ{drop_ratio:.2%}ï¼‰ï¼Œå°è¯•å›æ»š")
                        rollback_res = self.rollback_model(backup_index=-2)
                        if rollback_res["success"]:
                            # é‡æ–°åŠ è½½æ¨¡å‹
                            self._load_model()
                            # æ›´æ–°åŸºçº¿
                            if self.eval_history and len(self.eval_history) > 1:
                                self.baseline_rmse = self.eval_history[-2]["avg_rmse"]
                            logger.info(f"å›æ»šæˆåŠŸï¼ŒåŸºçº¿RMSEæ¢å¤ä¸ºï¼š{self.baseline_rmse}")
                else:
                    skip_reason = []
                    if eval_result["status"] != "success":
                        skip_reason.append("è¯„ä¼°å¤±è´¥")
                    if agg_rmse is None:
                        skip_reason.append("æ— RMSEæ•°æ®")
                    if self.baseline_rmse is None:
                        skip_reason.append("æ— åŸºçº¿æ•°æ®")
                    if len(df) <= 10:
                        skip_reason.append("å°æ•°æ®è®­ç»ƒ")
                    logger.info(f"è·³è¿‡æ€§èƒ½å›æ»šæ£€æŸ¥: {', '.join(skip_reason)}")

                # Step 13: å¦‚æœæ˜¯å¤§æ•°æ®é‡åˆå§‹è®­ç»ƒï¼Œè®¾ç½®å›ºå®šè¯„ä¼°é›†
                if len(df) >= 100 and not hasattr(self, 'fixed_evaluation_set'):
                    logger.info("å¤§æ•°æ®é‡è®­ç»ƒï¼Œè®¾ç½®å›ºå®šè¯„ä¼°é›†")
                    self.set_fixed_evaluation_set(df, size=50)

                # Step 12: è®°å½•è®­ç»ƒå†å²åˆ°æ•°æ®åº“
                train_duration = (datetime.now() - train_start).total_seconds()
                # ç›‘æ§è°ƒç”¨
                from performance_monitor import global_monitor
                global_monitor.record_training_session(
                    train_mode=reason,  # ä»åŸæœ‰å˜é‡è·å–
                    sample_count=len(df),  # ä»åŸæœ‰å˜é‡è·å–
                    duration=train_duration,
                    rmse=agg_rmse  # ä»åŸæœ‰å˜é‡è·å–
                )
                training_record = {
                    "sample_count": len(df),
                    "total_samples": self.total_samples,
                    "train_mode": reason,
                    "status": "success",
                    "message": f"è®­ç»ƒå®Œæˆï¼ˆ{reason}ï¼‰ï¼ŒRMSEï¼š{agg_rmse:.4f}" if agg_rmse else f"è®­ç»ƒå®Œæˆï¼ˆ{reason}ï¼‰ï¼ŒRMSEï¼šæœªè®¡ç®—",
                    "duration": train_duration,
                    "train_time": datetime.now()
                }
                record_id = self.db.insert_training_record(training_record)

                # æ„å»ºè¿”å›ç»“æœ
                training_result = {
                    "status": "success",
                    "message": f"è®­ç»ƒå®Œæˆï¼ˆæ¨¡å¼ï¼š{reason}ï¼‰",
                    "training_stats": {
                        "processed_samples": len(df),
                        "saved_to_db": saved_count,
                        "new_samples": new_samples,
                        "total_samples": self.total_samples,
                        "training_mode": reason,
                        "evaluation_rmse": agg_rmse,
                        "training_duration": round(train_duration, 2),
                        "record_id": record_id
                    },
                    "evaluation_details": eval_result if eval_result.get("status") == "success" else None
                }
                rmse_str = f"{agg_rmse:.4f}" if agg_rmse is not None else "æ— è¯„ä¼°æ•°æ®"
                logger.info(f"è®­ç»ƒåè¯„ä¼°RMSE: {rmse_str}")
                # ============ åªåœ¨è®­ç»ƒæˆåŠŸå®Œæˆåæ·»åŠ è¯Šæ–­ä¿¡æ¯è¾“å‡º ============
                if training_result.get("status") == "success":
                    # è¾“å‡ºè®­ç»ƒè¯Šæ–­ä¿¡æ¯
                    self._print_training_diagnosis()

                return training_result

            except Exception as e:
                # è®­ç»ƒå¤±è´¥ â†’ å›æ»šäº‹åŠ¡
                # å…³é”®ï¼šstr(e) å¯èƒ½è¢«å¤–å±‚ KeyError ç­‰è¦†ç›–ï¼Œå¢åŠ  repr(e) ä¾¿äºå®šä½çœŸå®æ ¹å› 
                logger.error(f"è®­ç»ƒå¤±è´¥ï¼Œè§¦å‘å›æ»šï¼ˆstrï¼‰ï¼š{str(e)}", exc_info=True)
                logger.error(f"è®­ç»ƒå¤±è´¥ï¼Œè§¦å‘å›æ»šï¼ˆreprï¼‰ï¼š{repr(e)}", exc_info=True)
                # è®­ç»ƒå¤±è´¥æ—¶ä¹Ÿè®°å½•ç›‘æ§
                train_duration = (datetime.now() - train_start).total_seconds()
                from performance_monitor import global_monitor
                global_monitor.record_training_session(
                    train_mode="failed",
                    sample_count=len(df) if 'df' in locals() else 0,
                    duration=train_duration,
                    rmse=None
                )
                if db_trans:
                    try:
                        db_trans.rollback()
                        logger.info("äº‹åŠ¡å·²å›æ»šï¼Œæ— æ®‹ç•™æ•°æ®")
                    except Exception as rollback_e:
                        if custom_create_time and db_conn:
                            from sqlalchemy import text
                            delete_sql = text("DELETE FROM t_prediction_parameters WHERE create_time = :ct")
                            db_conn.execute(delete_sql, {"ct": custom_create_time})
                            db_trans.commit()
                            logger.info(f"æ‰‹åŠ¨åˆ é™¤æ®‹ç•™æ•°æ®ï¼ˆcreate_timeï¼š{custom_create_time}ï¼‰")
                return {
                    "status": "error",
                    # è¿”å› message åŒæ—¶å¸¦ä¸Š repr(e)ï¼Œé¿å…åªçœ‹åˆ° "'pred_id'" è¿™ç§è¢«è¦†ç›–çš„ä¿¡æ¯
                    "message": f"{str(e)} | {repr(e)}",
                    "training_stats": {
                        "processed_samples": len(df) if 'df' in locals() else 0,
                        "saved_to_db": saved_count,
                        "data_rolled_back": True
                    }
                }
            finally:
                if db_conn:
                    try:
                        db_conn.close()
                        logger.debug("è®­ç»ƒæµç¨‹æ•°æ®åº“è¿æ¥å·²å…³é—­")
                    except Exception as close_e:
                        logger.warning(f"å…³é—­è¿æ¥å¤±è´¥ï¼š{str(close_e)}")

    def evaluate_model(self, eval_size=200, eval_df=None, use_fixed_set=False):
        """
        å¢å¼ºçš„æ¨¡å‹è¯„ä¼°æ–¹æ³•ï¼Œæ”¯æŒå›ºå®šè¯„ä¼°é›†

        :param eval_size: è¯„ä¼°æ ·æœ¬æ•°
        :param eval_df: å¤–éƒ¨è¯„ä¼°æ•°æ®
        :param use_fixed_set: æ˜¯å¦ä½¿ç”¨å›ºå®šè¯„ä¼°é›†
        :return: è¯„ä¼°ç»“æœ
        """
        if use_fixed_set and hasattr(self, 'fixed_evaluation_set') and self.fixed_evaluation_set is not None:
            logger.info("ä½¿ç”¨å›ºå®šè¯„ä¼°é›†è¿›è¡Œè¯„ä¼°")
            eval_df = self.fixed_evaluation_set

        return self.model_evaluator.evaluate_model(
            self.models, self.preprocessor, self.training_features, self.data_preprocessor.target_features,
            self._fitted_feature_order, self.db, eval_size, eval_df,
            data_preprocessor=self.data_preprocessor
        )

    def predict(self, data):
        """æ¨¡å‹é¢„æµ‹ï¼ˆå§”æ‰˜ç»™ModelPredictorï¼‰"""
        return self.model_predictor.predict(
            data, self.models, self.preprocessor, self.training_features,
            self.data_preprocessor.target_features, self._fitted_feature_order, self.is_trained,
            self.file_lock, self.data_preprocessor, self.fault_calculator, self.db
        )

    def retrain_from_db(self, workface_id=None, limit=None):
        """ä»æ•°æ®åº“é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰"""
        self._print_header("ä»æ•°æ®åº“é‡æ–°è®­ç»ƒæ¨¡å‹")

        # è®°å½•å‘åå…¼å®¹çš„è­¦å‘Š
        logger.warning("retrain_from_dbæ–¹æ³•å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨retrain_from_db_fullæ–¹æ³•")

        try:
            # ä»æ•°æ®åº“è¯»å–å†å²æ•°æ®
            df = self.model_manager.get_recent_data_from_db(self.db, limit=limit)
            if df.empty:
                msg = "æœªä»æ•°æ®åº“è¯»å–åˆ°ä»»ä½•æ•°æ®ï¼Œæ— æ³•é‡æ–°è®­ç»ƒ"
                logger.warning(msg)
                self._print_result(msg)
                return {"status": "warning", "message": msg}

            # ç­›é€‰ç‰¹å®šå·¥ä½œé¢æ•°æ®
            if workface_id is not None and 'workface_id' in df.columns:
                df = df[df["workface_id"] == workface_id].reset_index(drop=True)
                self._print_step(f"ç­›é€‰å·¥ä½œé¢ID={workface_id}ï¼Œå‰©ä½™æ ·æœ¬æ•°ï¼š{len(df)}")
                if df.empty:
                    msg = f"å·¥ä½œé¢ID={workface_id} æ— æ•°æ®"
                    logger.warning(msg)
                    return {"status": "warning", "message": msg}

            # é‡æ–°è®¡ç®—æ–­å±‚ç³»æ•°
            logger.info("é‡æ–°è®­ç»ƒï¼šè‡ªåŠ¨è®¡ç®—æ–­å±‚å½±å“ç³»æ•°")
            df = self.calculate_fault_influence_strength(df)

            # æ‰§è¡Œè®­ç»ƒï¼ˆä½¿ç”¨æ–°çš„å…¨é‡é‡æ–°è®­ç»ƒæ–¹æ³•ï¼‰
            result = self.retrain_from_db_full(
                workface_id=workface_id,
                sample_limit=limit,
                force_full_train=True
            )
            return result
        except Exception as e:
            logger.error(f"é‡æ–°è®­ç»ƒå¤±è´¥ï¼š{str(e)}", exc_info=True)
            return {"status": "error", "message": str(e)}

    def rollback_model(self, backup_index=-1):
        """æ¨¡å‹å›æ»šï¼ˆå§”æ‰˜ç»™ModelManagerï¼‰"""
        result = self.model_manager.rollback_model(backup_index, self.data_preprocessor.target_features)
        if result["success"]:
            # é‡æ–°åŠ è½½æ¨¡å‹
            self._load_model()
            # åŒæ­¥çŠ¶æ€åˆ°é¢„å¤„ç†å™¨
            self.data_preprocessor.is_trained = self.is_trained
            self.data_preprocessor.training_features = self.training_features
        return result

    def get_model_status(self):
        """è·å–æ¨¡å‹å½“å‰çŠ¶æ€"""
        backup_count = 0
        backup_root = os.path.join(self.model_dir, "backup")
        if os.path.exists(backup_root):
            try:
                backup_count = len(os.listdir(backup_root))
            except Exception:
                backup_count = 0

        latest_eval = self.eval_history[-1] if self.eval_history else None

        return {
            "is_trained": self.is_trained,
            "total_samples": self.total_samples,
            "training_features_count": len(self.training_features) if self.training_features else 0,
            "target_features": self.data_preprocessor.target_features,
            "backup_count": backup_count,
            "latest_evaluation": latest_eval,
            "last_train_time": self.training_stats[-1]["timestamp"] if self.training_stats else None
        }

    def _save_training_stats(self, train_mode, sample_count, agg_rmse):
        """ä¿å­˜è®­ç»ƒç»Ÿè®¡åˆ°å†…å­˜"""
        if not hasattr(self, 'training_stats'):
            self.training_stats = []
        self.training_stats.append({
            "timestamp": datetime.now(),
            "train_mode": train_mode,
            "sample_count": sample_count,
            "total_samples": self.total_samples,
            "agg_rmse": agg_rmse
        })
        self.training_stats = self.training_stats[-100:]
        logger.debug(f"è®­ç»ƒç»Ÿè®¡æ›´æ–°ï¼Œç´¯è®¡ {len(self.training_stats)} æ¡è®°å½•")

    def create_fixed_evaluation_set(self, data, size=50):
        """
        åˆ›å»ºå›ºå®šçš„è¯„ä¼°æ•°æ®é›†
        ç¡®ä¿ä¸åŒè®­ç»ƒé˜¶æ®µä½¿ç”¨ç›¸åŒçš„è¯„ä¼°åŸºå‡†

        :param data: è®­ç»ƒæ•°æ®
        :param size: è¯„ä¼°é›†å¤§å°
        :return: å›ºå®šè¯„ä¼°æ•°æ®é›†
        """
        try:
            if isinstance(data, list):
                df = pd.DataFrame(data)
            else:
                df = data.copy()

            # ç¡®ä¿æ•°æ®é‡è¶³å¤Ÿ
            if len(df) < size:
                logger.warning(f"æ•°æ®é‡ä¸è¶³({len(df)}æ¡)ï¼Œæ— æ³•åˆ›å»º{size}æ¡çš„å›ºå®šè¯„ä¼°é›†")
                return df

            # æŒ‰å·¥ä½œé¢åˆ†å±‚é‡‡æ ·ï¼Œç¡®ä¿è¯„ä¼°é›†ä»£è¡¨æ€§
            fixed_eval_set = []
            if 'workface_id' in df.columns:
                workface_groups = df.groupby('workface_id')
                for workface_id, group in workface_groups:
                    group_size = max(1, int(size * len(group) / len(df)))
                    if len(group) >= group_size:
                        sampled = group.sample(n=group_size, random_state=42)
                        fixed_eval_set.append(sampled)

                if fixed_eval_set:
                    fixed_eval_df = pd.concat(fixed_eval_set, ignore_index=True)
                    # å¦‚æœæ€»æ•°è¶…è¿‡sizeï¼Œéšæœºé‡‡æ ·è°ƒæ•´
                    if len(fixed_eval_df) > size:
                        fixed_eval_df = fixed_eval_df.sample(n=size, random_state=42)
                else:
                    fixed_eval_df = df.sample(n=size, random_state=42)
            else:
                fixed_eval_df = df.sample(n=size, random_state=42)

            logger.info(f"åˆ›å»ºå›ºå®šè¯„ä¼°é›†: {len(fixed_eval_df)}æ¡æ•°æ®")
            return fixed_eval_df

        except Exception as e:
            logger.error(f"åˆ›å»ºå›ºå®šè¯„ä¼°é›†å¤±è´¥: {str(e)}")
            # å¤±è´¥æ—¶å›é€€åˆ°éšæœºé‡‡æ ·
            return df.sample(n=min(size, len(df)), random_state=42)

    def set_fixed_evaluation_set(self, data, size=50):
        """
        è®¾ç½®å›ºå®šè¯„ä¼°æ•°æ®é›†ä¾›åç»­ä½¿ç”¨
        """
        self.fixed_evaluation_set = self.create_fixed_evaluation_set(data, size)
        logger.info(f"å›ºå®šè¯„ä¼°é›†å·²è®¾ç½®: {len(self.fixed_evaluation_set)}æ¡æ•°æ®")
        return self.fixed_evaluation_set

    def retrain_from_db_full(self, workface_id=None, sample_limit=None, force_full_train=True):
        """
        å…¨é‡é‡æ–°è®­ç»ƒæ–¹æ³•ï¼ˆé˜²æ­¢æ¨¡å‹è¢«è¯¯åˆ é™¤ï¼Œå¼ºåˆ¶å…¨é‡è®­ç»ƒï¼‰

        :param workface_id: intï¼Œå¯é€‰ï¼Œç­›é€‰ç‰¹å®šå·¥ä½œé¢æ•°æ®
        :param sample_limit: intï¼Œå¯é€‰ï¼Œé™åˆ¶è®­ç»ƒæ ·æœ¬æ•°ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
        :param force_full_train: boolï¼Œæ˜¯å¦å¼ºåˆ¶å…¨é‡è®­ç»ƒï¼ˆé»˜è®¤Trueï¼‰
        :return: dictï¼Œè®­ç»ƒç»“æœ
        """
        with self.file_lock:
            self._print_header("å…¨é‡é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆä»æ•°æ®åº“æ¢å¤ï¼‰")
            retrain_start = datetime.now()

            try:
                # Step 1: ä»æ•°æ®åº“è¯»å–å†å²æ•°æ®ï¼ˆä½¿ç”¨æ¨¡å‹ç®¡ç†å™¨ï¼‰
                logger.info(f"ä»æ•°æ®åº“è¯»å–å†å²æ•°æ®ï¼šå·¥ä½œé¢å¯¹{workface_id}ï¼Œæ ·æœ¬é™åˆ¶{sample_limit}")
                df = self.model_manager.get_recent_data_from_db(self.db, limit=sample_limit)

                if df.empty:
                    msg = "æœªä»æ•°æ®åº“è¯»å–åˆ°ä»»ä½•æ•°æ®ï¼Œæ— æ³•é‡æ–°è®­ç»ƒ"
                    logger.warning(msg)
                    self._print_result(msg)
                    return {
                        "status": "warning",
                        "message": msg,
                        "training_stats": {"processed_samples": 0, "training_performed": False}
                    }

                # Step 2: ç­›é€‰ç‰¹å®šå·¥ä½œé¢æ•°æ®
                if workface_id is not None and 'workface_id' in df.columns:
                    original_count = len(df)
                    df = df[df["workface_id"] == workface_id].reset_index(drop=True)
                    if df.empty:
                        msg = f"å·¥ä½œé¢ID={workface_id} æ— æ•°æ®"
                        logger.warning(msg)
                        return {"status": "warning", "message": msg}
                    logger.info(f"ç­›é€‰å·¥ä½œé¢ID={workface_id}ï¼Œæ ·æœ¬æ•°ï¼š{original_count} â†’ {len(df)}")

                # Step 3: æ•°æ®é¢„å¤„ç†ï¼ˆé‡æ–°è®¡ç®—æ–­å±‚å½±å“ç³»æ•°ï¼‰
                logger.info("å…¨é‡é‡æ–°è®­ç»ƒï¼šè‡ªåŠ¨è®¡ç®—æ–­å±‚å½±å“ç³»æ•°")
                try:
                    df = self.calculate_fault_influence_strength(df)
                except Exception as e:
                    logger.error(f"è®¡ç®—æ–­å±‚å½±å“ç³»æ•°å¤±è´¥ï¼š{str(e)}ï¼Œä½¿ç”¨ç°æœ‰å€¼")
                    # å³ä½¿å¤±è´¥ä¹Ÿç»§ç»­ï¼Œä½¿ç”¨ç°æœ‰å€¼

                # Step 4: æ•°æ®é¢„å¤„ç†ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰
                try:
                    df_processed, training_features = self.data_preprocessor.preprocess_data(
                        df, is_training=True, fault_calculator=self.fault_calculator, db_utils=self.db
                    )
                except Exception as e:
                    logger.error(f"æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼š{str(e)}")
                    raise ValueError(f"æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼š{str(e)}")

                # Step 5: æ£€æŸ¥æœ€å°æ ·æœ¬æ•°
                if len(df_processed) < self.min_train_samples:
                    msg = f"æ ·æœ¬æ•° {len(df_processed)} < æœ€å°è®­ç»ƒæ ·æœ¬æ•° {self.min_train_samples}ï¼Œæ— æ³•é‡æ–°è®­ç»ƒ"
                    logger.warning(msg)
                    self._print_result(msg)
                    return {
                        "status": "warning",
                        "message": msg,
                        "training_stats": {"processed_samples": len(df_processed), "training_performed": False}
                    }

                # Step 6: å¼ºåˆ¶é…ç½®ä¸ºå…¨é‡è®­ç»ƒæ¨¡å¼
                logger.info("å¼ºåˆ¶ä½¿ç”¨å…¨é‡è®­ç»ƒé…ç½®")
                # åˆ‡æ¢åˆ°å…¨é‡è®­ç»ƒé…ç½®ï¼ˆphase1ï¼‰
                config_before = self.current_config
                if self.current_config != "config_phase1.ini":
                    logger.info(f"åˆ‡æ¢åˆ°å…¨é‡è®­ç»ƒé…ç½®ï¼š{config_before} â†’ config_phase1.ini")
                    self.reload_config("config_phase1.ini", reload_database=False)

                # Step 7: æ„å»ºè®­ç»ƒé›†
                X = df_processed[training_features]
                y = df_processed[self.data_preprocessor.target_features].values
                fitted_feature_order = X.columns.tolist()

                # Step 8: ç‰¹å¾é¢„å¤„ç†
                preprocessor, X_proc, _ = self.model_trainer.create_preprocessor(
                    X, self.data_preprocessor.base_categorical
                )

                # Step 9: æ‰§è¡Œå…¨é‡è®­ç»ƒï¼ˆå¼ºåˆ¶ä½¿ç”¨å…¨é‡è®­ç»ƒé€»è¾‘ï¼‰
                logger.info(
                    f"å¼€å§‹å…¨é‡è®­ç»ƒï¼Œæ ·æœ¬æ•°ï¼š{len(df_processed)}ï¼Œç›®æ ‡æ•°ï¼š{len(self.data_preprocessor.target_features)}")
                models = self.model_trainer._full_train(X_proc, y, self.data_preprocessor.target_features)

                # Step 10: æ›´æ–°æ¨¡å‹çŠ¶æ€
                self.models = models
                self.preprocessor = preprocessor
                self.training_features = training_features
                self._fitted_feature_order = fitted_feature_order
                self.is_trained = True
                self.data_preprocessor.is_trained = True
                self.data_preprocessor.training_features = training_features

                # Step 11: ä¿å­˜æ¨¡å‹ï¼ˆä¸è§¦å‘å¤‡ä»½ï¼Œå› ä¸ºæ˜¯æ¢å¤è®­ç»ƒï¼‰
                logger.info("ä¿å­˜é‡æ–°è®­ç»ƒçš„æ¨¡å‹")
                self.model_manager.save_model(self.models, self.preprocessor, self.training_features,
                                              self.data_preprocessor.target_features)

                # Step 12: æ¨¡å‹è¯„ä¼°
                eval_result = None
                if len(df_processed) > 5:  # é¿å…å°æ•°æ®è¯„ä¼°ä¸å‡†ç¡®
                    try:
                        eval_result = self.evaluate_model(eval_size=min(50, len(df_processed)), eval_df=df_processed)
                        logger.info(
                            f"é‡æ–°è®­ç»ƒåè¯„ä¼°ç»“æœï¼šçŠ¶æ€={eval_result.get('status')}, RMSE={eval_result.get('avg_rmse')}")
                    except Exception as e:
                        logger.warning(f"é‡æ–°è®­ç»ƒåè¯„ä¼°å¤±è´¥ï¼š{str(e)}")

                # Step 13: æ¢å¤åŸé…ç½®ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if config_before != "config_phase1.ini":
                    logger.info(f"æ¢å¤åŸé…ç½®ï¼šconfig_phase1.ini â†’ {config_before}")
                    self.reload_config(config_before, reload_database=False)

                # Step 14: è®¡ç®—è®­ç»ƒè€—æ—¶
                train_duration = (datetime.now() - retrain_start).total_seconds()

                # Step 15: è®°å½•ç›‘æ§
                from performance_monitor import global_monitor
                global_monitor.record_training_session(
                    train_mode="full_retrain",  # ç‰¹æ®Šæ ‡è®°ä¸ºå…¨é‡é‡æ–°è®­ç»ƒ
                    sample_count=len(df_processed),
                    duration=train_duration,
                    rmse=eval_result.get("avg_rmse") if eval_result else None
                )

                # Step 16: è®°å½•è®­ç»ƒå†å²
                training_record = {
                    "sample_count": len(df_processed),
                    "total_samples": self.total_samples,  # æ³¨æ„ï¼šä¸æ›´æ–°æ€»æ ·æœ¬æ•°
                    "train_mode": "full_retrain",
                    "status": "success",
                    "message": f"å…¨é‡é‡æ–°è®­ç»ƒå®Œæˆï¼Œæ ·æœ¬æ•°ï¼š{len(df_processed)}ï¼ŒRMSEï¼š{eval_result.get('avg_rmse') if eval_result else 'æœªè¯„ä¼°'}",
                    "duration": train_duration,
                    "train_time": datetime.now()
                }
                record_id = self.db.insert_training_record(training_record)

                # Step 17: æ„å»ºè¿”å›ç»“æœ
                training_result = {
                    "status": "success",
                    "message": f"å…¨é‡é‡æ–°è®­ç»ƒå®Œæˆï¼ˆæ ·æœ¬æ•°ï¼š{len(df_processed)}ï¼‰",
                    "training_stats": {
                        "processed_samples": len(df_processed),
                        "training_mode": "full_retrain",
                        "evaluation_rmse": eval_result.get("avg_rmse") if eval_result else None,
                        "training_duration": round(train_duration, 2),
                        "record_id": record_id,
                        "workface_filtered": workface_id is not None,
                        "sample_limit_applied": sample_limit is not None
                    },
                    "evaluation_details": eval_result if eval_result and eval_result.get(
                        "status") == "success" else None
                }

                # è¾“å‡ºè®­ç»ƒè¯Šæ–­ä¿¡æ¯
                if training_result.get("status") == "success":
                    self._print_training_diagnosis()

                logger.info(f"å…¨é‡é‡æ–°è®­ç»ƒæˆåŠŸå®Œæˆï¼Œè€—æ—¶ï¼š{train_duration:.2f}ç§’ï¼Œæ ·æœ¬æ•°ï¼š{len(df_processed)}")
                return training_result

            except Exception as e:
                # è®­ç»ƒå¤±è´¥å¤„ç†
                train_duration = (datetime.now() - retrain_start).total_seconds()
                logger.error(f"å…¨é‡é‡æ–°è®­ç»ƒå¤±è´¥ï¼š{str(e)}", exc_info=True)

                # è®°å½•ç›‘æ§
                from performance_monitor import global_monitor
                global_monitor.record_training_session(
                    train_mode="full_retrain_failed",
                    sample_count=len(df) if 'df' in locals() else 0,
                    duration=train_duration,
                    rmse=None
                )

                return {
                    "status": "error",
                    "message": str(e),
                    "training_stats": {
                        "processed_samples": len(df) if 'df' in locals() else 0,
                        "training_duration": round(train_duration, 2),
                        "training_performed": False
                    }
                }