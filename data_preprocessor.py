"""
ç…¤çŸ¿ç“¦æ–¯é£é™©é¢„æµ‹ç³»ç»Ÿ - æ•°æ®é¢„å¤„ç†æ¨¡å—
åŒ…å«ï¼šæ•°æ®é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹ã€åˆ†æºç‰¹å¾è®¡ç®—
"""
from datetime import datetime

import pandas as pd
import numpy as np
from loguru import logger
from sqlalchemy import create_engine

from config_utils import ConfigUtils


class DataPreprocessor(ConfigUtils):
    """æ•°æ®é¢„å¤„ç†å™¨"""

    def __init__(self, config_path="config.ini"):
        super().__init__(config_path)
        self.fixed_reference_date = None
        self.date_reference = None
        self.target_features = None
        self._load_feature_config()  # åœ¨åˆå§‹åŒ–æ—¶åŠ è½½ç‰¹å¾é…ç½®

        # ============ æ–°å¢ï¼šåŠ è½½è¿›å°ºé…ç½® ============
        self._load_mining_advance_config()
        self.spatiotemporal_extractor = None
        self._init_spatiotemporal_extractor(config_path)
        # ============ æ–°å¢ç»“æŸ ============

        # æ·»åŠ ç¼ºå¤±çš„å±æ€§
        self.is_trained = False
        self.training_features = None

    def _init_spatiotemporal_extractor(self, config_path):
        """
        åˆå§‹åŒ–æ—¶ç©ºç‰¹å¾æå–å™¨
        """
        try:
            self.spatiotemporal_extractor = SpatiotemporalFeatureExtractor(config_path)
            logger.info("æ—¶ç©ºç‰¹å¾æå–å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"åˆå§‹åŒ–æ—¶ç©ºç‰¹å¾æå–å™¨å¤±è´¥: {str(e)}ï¼Œå°†ä¸ä½¿ç”¨æ—¶ç©ºç‰¹å¾")
            self.spatiotemporal_extractor = None

    def _load_mining_advance_config(self):
        """
        ç§æœ‰æ–¹æ³•ï¼šåŠ è½½å›é‡‡è¿›å°ºç›¸å…³é…ç½®ï¼ˆ[MiningAdvance] sectionï¼‰
        """
        try:
            self.default_daily_advance = self._get_config_value(
                "MiningAdvance", "default_daily_advance", 3.0, is_float=True
            )

            # ä½¿ç”¨config.getbooleanè¯»å–å¸ƒå°”å€¼
            self.enable_cumulative_advance = self.config.getboolean(
                "MiningAdvance", "enable_cumulative_advance", fallback=True
            )
            self.enable_effective_exposure = self.config.getboolean(
                "MiningAdvance", "enable_effective_exposure", fallback=True
            )

            self.advance_data_gap_threshold = self._get_config_value(
                "MiningAdvance", "advance_data_gap_threshold", 3, is_int=True
            )

            logger.info(
                f"å›é‡‡è¿›å°ºé…ç½®åŠ è½½å®Œæˆï¼šé»˜è®¤æ—¥è¿›å°º={self.default_daily_advance}mï¼Œ"
                f"ç´¯è®¡è¿›å°ºç‰¹å¾={'å¯ç”¨' if self.enable_cumulative_advance else 'ç¦ç”¨'}ï¼Œ"
                f"æœ‰æ•ˆæš´éœ²è·ç¦»ç‰¹å¾={'å¯ç”¨' if self.enable_effective_exposure else 'ç¦ç”¨'}"
            )

        except Exception as e:
            logger.warning(f"åŠ è½½å›é‡‡è¿›å°ºé…ç½®å¤±è´¥ï¼š{str(e)}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            self._set_default_mining_advance_params()

    def _set_default_mining_advance_params(self):
        """ç§æœ‰æ–¹æ³•ï¼šå›é‡‡è¿›å°ºé»˜è®¤å‚æ•°ï¼ˆé…ç½®ç¼ºå¤±æ—¶å…œåº•ï¼‰"""
        self.default_daily_advance = 3.0  # é»˜è®¤æ¯æ—¥è¿›å°º3ç±³
        self.enable_cumulative_advance = True
        self.enable_effective_exposure = True
        self.advance_data_gap_threshold = 3
        logger.debug("å›é‡‡è¿›å°ºå·²è®¾ç½®é»˜è®¤å‚æ•°")


    def preprocess_data(self, data, is_training=True, fault_calculator=None, db_utils=None):
        """
        å…¬å¼€æ–¹æ³•ï¼šæ•°æ®é¢„å¤„ç†ï¼ˆç»Ÿä¸€æ ¼å¼ã€æ¸…æ´—ã€ç‰¹å¾ç”Ÿæˆï¼Œå…¼å®¹è®­ç»ƒ/é¢„æµ‹æ¨¡å¼ï¼‰

        :param data: list[dict] / pandas.DataFrameï¼Œè¾“å…¥æ•°æ®
        :param is_training: boolï¼Œæ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼ï¼ˆTrue=è®­ç»ƒï¼ŒFalse=é¢„æµ‹/è¯„ä¼°ï¼‰
        :param fault_calculator: FaultCalculatorå®ä¾‹ï¼Œç”¨äºæ–­å±‚è®¡ç®—
        :param db_utils: DBUtilså®ä¾‹ï¼Œç”¨äºæ•°æ®åº“æ“ä½œ
        :return: pandas.DataFrameï¼Œé¢„å¤„ç†åçš„æ•°æ®
        :raises ValueError: å…³é”®ç‰¹å¾ç¼ºå¤±æ—¶æŠ›å‡º
        """
        logger.debug(f"æ•°æ®é¢„å¤„ç†å¼€å§‹ï¼ˆè®­ç»ƒæ¨¡å¼: {'æ˜¯' if is_training else 'å¦'}ï¼‰ï¼ŒåŸå§‹æ ·æœ¬: {len(data)}")

        # Step 1: ç»Ÿä¸€æ•°æ®æ ¼å¼ä¸ºDataFrame
        if isinstance(data, pd.DataFrame):
            df = data.copy()
        else:
            df = pd.DataFrame(data)
        logger.debug(f"æ•°æ®æ ¼å¼ç»Ÿä¸€ä¸ºDataFrameï¼Œåˆå§‹æ ·æœ¬æ•°ï¼š{len(df)}")
        # ============ è¿›å°ºç‰¹å¾å¤„ç†ï¼ˆåœ¨æ—¶é—´ç‰¹å¾ä¹‹å‰ï¼‰ ============
        if self.enable_cumulative_advance or self.enable_effective_exposure:
            df = self._process_mining_advance_features(df, is_training, db_utils)
        # æ—¶é—´ç‰¹å¾å¤„ç†ï¼ˆåœ¨æ—©æœŸå¤„ç†ä»¥ç¡®ä¿åç»­å¯ç”¨ï¼‰
        if self.enable_temporal_features:
            df = self._process_temporal_features_simple(df)
        if self.spatiotemporal_extractor:
            try:
                logger.debug("å¼€å§‹æå–æ—¶ç©ºç‰¹å¾")
                df = self.spatiotemporal_extractor.extract_features(df, is_training)
                new_features = self.spatiotemporal_extractor.get_new_feature_names()
                if new_features:
                    logger.info(f"æ—¶ç©ºç‰¹å¾æå–å®Œæˆï¼Œæ–°å¢ {len(new_features)} ä¸ªç‰¹å¾")
                    logger.debug(
                        f"æ–°å¢ç‰¹å¾: {', '.join(new_features[:10])}{'...' if len(new_features) > 10 else ''}")
            except Exception as e:
                logger.error(f"æå–æ—¶ç©ºç‰¹å¾æ—¶å‡ºé”™: {str(e)}", exc_info=True)
        else:
            logger.debug("æœªå¯ç”¨æ—¶ç©ºç‰¹å¾æå–å™¨")

        # Step 2: è‡ªåŠ¨è¡¥å……æ–­å±‚å½±å“ç³»æ•°ï¼ˆç¼ºå¤±æ—¶è®¡ç®—ï¼‰
        if 'fault_influence_strength' not in df.columns or df['fault_influence_strength'].isnull().any():
            logger.debug("æ£€æµ‹åˆ° fault_influence_strength ç¼ºå¤±ï¼Œè‡ªåŠ¨è®¡ç®—")
            if fault_calculator and db_utils:
                df_dict = fault_calculator.calculate_fault_influence_strength(df.to_dict('records'), db_utils)
                df = pd.DataFrame(df_dict)
            else:
                logger.warning("ç¼ºå°‘æ–­å±‚è®¡ç®—å™¨æˆ–æ•°æ®åº“å·¥å…·ï¼Œæ— æ³•è®¡ç®—æ–­å±‚å½±å“ç³»æ•°")
                # è®¾ç½®é»˜è®¤å€¼
                df['fault_influence_strength'] = 0.5

        # Step 3: æ ¡éªŒåŒºåŸŸæªæ–½å¼ºåº¦ï¼ˆè®­ç»ƒ/é¢„æµ‹å‡éœ€æå‰è®¡ç®—ï¼‰
        if 'regional_measure_strength' not in df.columns or df['regional_measure_strength'].isnull().any():
            raise ValueError(
                "æ•°æ®ç¼ºå°‘ regional_measure_strengthï¼éœ€å…ˆè°ƒç”¨ /api/model/calculate_regional_strength æ¥å£è®¡ç®—"
            )

        # Step 4: åˆ—åæ ‡å‡†åŒ–ï¼ˆå¤„ç†ç©ºæ ¼/æ¨ªæ ï¼Œé¿å…å­—æ®µåŒ¹é…é”™è¯¯ï¼‰
        df.columns = (
            df.columns
            .astype(str)
            .str.strip()
            .str.replace(' ', '_')
            .str.replace('-', '_')
        )
        logger.debug(f"åˆ—åæ ‡å‡†åŒ–å®Œæˆï¼Œå½“å‰åˆ—ï¼š{df.columns.tolist()}")

        # ä¿®æ­£ç›®æ ‡åˆ—åæ˜ å°„ï¼ˆç¡®ä¿ä¸æ ‡å‡†åŒ–åçš„DataFrameä¸€è‡´ï¼‰
        normalized_cols = df.columns.tolist()
        fixed_targets = []
        for t in self.target_features:
            for c in normalized_cols:
                if c.lower() == t.lower():
                    fixed_targets.append(c)
                    break
        if set(fixed_targets) != set(self.target_features):
            logger.warning(f"ç›®æ ‡åˆ—åè‡ªåŠ¨ä¿®æ­£ï¼š{self.target_features} â†’ {fixed_targets}")
        self.target_features = fixed_targets

        # Step 5: æ•°æ®å»é‡ï¼ˆåŸºäºä½ç½®æ ‡è¯†ï¼Œä¿ç•™æœ€æ–°è®°å½•ï¼‰
        identifier_cols = ['working_face', 'roadway', 'roadway_id', 'distance_from_entrance']
        available_identifiers = [col for col in identifier_cols if col in df.columns]
        if available_identifiers:
            dup_cnt = df.duplicated(subset=available_identifiers).sum()
            if dup_cnt > 0:
                df = df.drop_duplicates(subset=available_identifiers, keep='last').reset_index(drop=True)
                logger.debug(f"æ•°æ®å»é‡ï¼šç§»é™¤ {dup_cnt} æ¡é‡å¤è®°å½•ï¼Œå‰©ä½™ {len(df)} æ¡")
        else:
            logger.warning("ä½ç½®æ ‡è¯†åˆ—ï¼ˆworking_face/roadwayç­‰ï¼‰ä¸å…¨ï¼Œæ— æ³•å®Œæ•´å»é‡")

        # Step 6: æŒ‰æ˜è¿›è·ç¦»æ’åºï¼ˆç¡®ä¿æ—¶åºé€»è¾‘ï¼Œdistance_from_entrance å‡åºï¼‰
        if 'distance_from_entrance' in df.columns:
            try:
                if pd.api.types.is_numeric_dtype(df['distance_from_entrance']):
                    df = df.sort_values(by='distance_from_entrance', ascending=True).reset_index(drop=True)
                    logger.debug("æŒ‰ distance_from_entrance å‡åºæ’åºå®Œæˆ")
                else:
                    logger.warning("distance_from_entrance éæ•°å€¼ç±»å‹ï¼Œè·³è¿‡æ’åº")
            except Exception as e:
                logger.warning(f"æ’åºå¤±è´¥ï¼š{str(e)}")

        # ============ ä¿®æ”¹ï¼šç¼ºå¤±å€¼å¡«å……éƒ¨åˆ†ï¼Œå¤„ç†Timestampç±»å‹ ============
        # Step 7: ç¼ºå¤±å€¼å¡«å……ï¼ˆåˆ†ç±»ç”¨ä¼—æ•°ï¼Œæ•°å€¼ç”¨ä¸­ä½æ•°ï¼Œé¿å…å¼‚å¸¸å€¼å¹²æ‰°ï¼‰
        # 7.1 åˆ†ç±»ç‰¹å¾å¡«å……ï¼ˆå¦‚working_faceï¼‰
        for col in self.base_categorical:
            if col in df.columns and df[col].isnull().any():
                fill_val = df[col].mode()[0] if not df[col].mode().empty else "æœªçŸ¥"
                df[col] = df[col].fillna(fill_val)
                logger.debug(f"åˆ†ç±»ç‰¹å¾ {col} ç¼ºå¤±å€¼å¡«å……ï¼š{fill_val}")

        # 7.2 æ•°å€¼ç‰¹å¾å¡«å……ï¼ˆå¦‚coal_thicknessï¼‰
        for col in self.base_numeric:
            if col in df.columns and df[col].isnull().any():
                # ç‰¹æ®Šå¤„ç†æ—¶é—´ç›¸å…³å­—æ®µ
                if col in ['depth_from_face']:
                    # éªŒè¯å­”æ·±åº¦ç¼ºå¤±æ—¶ä½¿ç”¨é»˜è®¤å€¼0ï¼ˆè¡¨ç¤ºå·¥ä½œé¢ä½ç½®ï¼‰
                    fill_val = 0.0
                    logger.debug(f"éªŒè¯å­”æ·±åº¦ {col} ç¼ºå¤±å€¼å¡«å……ï¼š{fill_val}")
                elif col == 'measurement_date':
                    # æµ‹é‡æ—¥æœŸç¼ºå¤±æ—¶ï¼Œä½¿ç”¨æ•°æ®ä¸­æœ€å¸¸è§çš„æ—¥æœŸæˆ–å½“å‰æ—¥æœŸ
                    # ç¡®ä¿ä¸ä¿ç•™Timestampå¯¹è±¡
                    if not df[col].mode().empty:
                        fill_val = df[col].mode()[0]
                        # å¦‚æœæ˜¯Timestampï¼Œè½¬æ¢ä¸ºdatetime
                        if isinstance(fill_val, pd.Timestamp):
                            fill_val = fill_val.to_pydatetime()
                    else:
                        # ä½¿ç”¨å½“å‰æ—¥æœŸ
                        fill_val = datetime.now()
                    logger.debug(f"æµ‹é‡æ—¥æœŸ {col} ç¼ºå¤±å€¼å¡«å……ï¼š{fill_val}")
                else:
                    # æ£€æŸ¥åˆ—çš„æ•°æ®ç±»å‹
                    if pd.api.types.is_numeric_dtype(df[col]):
                        fill_val = df[col].median() if not df[col].isna().all() else 0.0
                    elif pd.api.types.is_datetime64_any_dtype(df[col]):
                        # å¦‚æœæ˜¯æ—¥æœŸç±»å‹ï¼Œä½¿ç”¨æœ€å¸¸è§çš„æ—¥æœŸ
                        if not df[col].mode().empty:
                            fill_val = df[col].mode()[0]
                        else:
                            fill_val = datetime.now()
                    else:
                        # å…¶ä»–ç±»å‹ä½¿ç”¨0.0
                        fill_val = 0.0
                    logger.debug(f"æ•°å€¼ç‰¹å¾ {col} ç¼ºå¤±å€¼å¡«å……ï¼š{fill_val}")

                df[col] = df[col].fillna(fill_val)
        # 7.3 ç›®æ ‡ç‰¹å¾å¡«å……ï¼ˆè®­ç»ƒ/è¯„ä¼°æ—¶ç¡®ä¿æ— NaNï¼‰
        if is_training:
            for col in self.target_features:
                if col in df.columns and df[col].isnull().any():
                    fill_val = df[col].median() if not df[col].isna().all() else 0.0
                    df[col] = df[col].fillna(fill_val)
                    logger.debug(f"ç›®æ ‡ç‰¹å¾ {col} ç¼ºå¤±å€¼å¡«å……ï¼š{fill_val}")

        # Step 8: ç”Ÿæˆåˆ†æºç‰¹å¾ï¼ˆAQ1018â€”2006æ ‡å‡†ï¼Œå…¨å‚æ•°å­˜åœ¨ä¸”æ— ç¼ºå¤±æ‰è®¡ç®—ï¼‰
        source_prediction_params = [
            "coal_thickness", "tunneling_speed", "initial_gas_emission_strength", "roadway_length",
            "roadway_cross_section", "coal_density", "original_gas_content", "residual_gas_content"
        ]

        all_params_exist = all(param in df.columns for param in source_prediction_params)
        if all_params_exist:
            if df[source_prediction_params].isnull().any().any():
                all_params_exist = False
                logger.warning("åˆ†æºå‚æ•°å­˜åœ¨ç¼ºå¤±å€¼ï¼Œä¸è®¡ç®—åˆ†æºç‰¹å¾")

        if all_params_exist:
            df["gas_emission_wall"] = df.apply(
                lambda r: self._calculate_coal_wall_emission(
                    r["coal_thickness"], r["tunneling_speed"],
                    r["initial_gas_emission_strength"], r["roadway_length"]
                ), axis=1
            )
            df["gas_emission_fallen"] = df.apply(
                lambda r: self._calculate_fallen_coal_emission(
                    r["roadway_cross_section"], r["coal_density"],
                    r["tunneling_speed"], r["original_gas_content"], r["residual_gas_content"]
                ), axis=1
            )
            df["total_gas_emission"] = df["gas_emission_wall"] + df["gas_emission_fallen"]
            logger.debug(
                f"åˆ†æºç‰¹å¾è®¡ç®—å®Œæˆï¼šå¹³å‡æ€»ç“¦æ–¯æ¶Œå‡ºé‡ {df['total_gas_emission'].mean():.4f} mÂ³/min"
            )
        else:
            df["gas_emission_wall"] = 0.0
            df["gas_emission_fallen"] = 0.0
            df["total_gas_emission"] = 0.0
            logger.debug("åˆ†æºå‚æ•°ä¸å…¨ï¼Œåˆ†æºç‰¹å¾è®¾ä¸º0.0")

        # Step 9: ç¡®ä¿æ‰€æœ‰æœŸæœ›ç‰¹å¾å­˜åœ¨ï¼ˆç¼ºå¤±åˆ™å¡«å……é»˜è®¤å€¼ï¼‰
        all_features = self.base_categorical + self.base_numeric + [
            'gas_emission_wall', 'gas_emission_fallen', 'total_gas_emission'
        ]
        # ============ 20251218æ–°å¢ï¼šåŒ…å«æ—¶ç©ºç‰¹å¾ ============
        if self.spatiotemporal_extractor:
            new_features = self.spatiotemporal_extractor.get_new_feature_names()
            if new_features:
                all_features.extend(new_features)
                logger.debug(f"ç‰¹å¾åˆ—è¡¨æ‰©å±•ï¼ŒåŒ…å« {len(new_features)} ä¸ªæ—¶ç©ºç‰¹å¾")
        # ============ 20251218æ–°å¢ç»“æŸ ============
        for col in all_features:
            if col not in df.columns:
                fill_val = "æœªçŸ¥" if col in self.base_categorical else 0.0
                df[col] = fill_val
                logger.debug(f"ç‰¹å¾ {col} ç¼ºå¤±ï¼Œå¡«å……é»˜è®¤å€¼ï¼š{fill_val}")

        # Step 10: è®­ç»ƒ/é¢„æµ‹æ¨¡å¼å·®å¼‚åŒ–å¤„ç†
        if is_training:
            missing_targets = [t for t in self.target_features if t not in df.columns]
            if missing_targets:
                raise ValueError(f"è®­ç»ƒæ•°æ®ç¼ºå°‘ç›®æ ‡ç‰¹å¾ï¼š{missing_targets}")
            training_features = all_features
            logger.debug(f"è®­ç»ƒç‰¹å¾ç¡®å®šï¼šå…± {len(training_features)} ä¸ª")
            return df, training_features
        else:
            if not self.training_features:
                raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ³•ç¡®å®šé¢„æµ‹ç‰¹å¾é¡ºåº")
            keep_cols = self.training_features + self.target_features
            keep_cols = [col for col in keep_cols if col in df.columns]
            df = df[keep_cols]
            logger.debug(f"é¢„æµ‹æ•°æ®å¯¹é½ï¼šæŒ‰è®­ç»ƒç‰¹å¾é¡ºåºä¿ç•™ {len(df.columns)} ä¸ªå­—æ®µ")
        if is_training:
            self._log_data_quality_summary(df)
            return df
        return df

    def _load_feature_config(self):
        """
        ç§æœ‰æ–¹æ³•ï¼šåŠ è½½ç‰¹å¾é…ç½®ï¼ˆ[Features] sectionï¼‰
        ç®€åŒ–ç‰ˆï¼šç§»é™¤æ¨è¿›è·ç¦»ç›¸å…³é…ç½®ï¼Œä¸“æ³¨äºæµ‹é‡æ—¥æœŸå’Œæ·±åº¦
        """
        try:
            # Step 1: è¯»å–åˆ†ç±»ç‰¹å¾ï¼ˆå¦‚å·¥ä½œé¢ã€å··é“ï¼‰
            categorical_str = self.config.get("Features", "base_categorical", fallback="")
            self.base_categorical = [x.strip() for x in categorical_str.split(",") if x.strip()]

            # Step 2: è¯»å–æ•°å€¼ç‰¹å¾ï¼ˆå¦‚åæ ‡ã€ç…¤å±‚åšåº¦ã€æµ‹é‡æ—¥æœŸã€éªŒè¯å­”æ·±åº¦ï¼‰
            numeric_str = self.config.get("Features", "base_numeric", fallback="")
            self.base_numeric = [x.strip() for x in numeric_str.split(",") if x.strip()]

            # Step 3: è¯»å–é¢„æµ‹ç›®æ ‡ç‰¹å¾ï¼ˆå¦‚ç“¦æ–¯æ¶Œå‡ºé‡Qï¼‰
            target_str = self.config.get("Features", "target_features", fallback="")
            self.target_features = [x.strip() for x in target_str.split(",") if x.strip()]

            # Step 4: è¯»å–æ—¶é—´ç‰¹å¾é…ç½®ï¼ˆä»[TemporalFeatures] sectionï¼Œç®€åŒ–ç‰ˆï¼‰
            try:
                # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ—¶é—´ç‰¹å¾
                self.enable_temporal_features = self.config.getboolean(
                    "TemporalFeatures", "enable_temporal_features", fallback=True
                )

                # è¯»å–æ—¥æœŸå‚è€ƒåŸºå‡†é…ç½®
                self.date_reference = self.config.get(
                    "TemporalFeatures", "date_reference", fallback="min_date"
                )

                # è¯»å–å›ºå®šå‚è€ƒæ—¥æœŸï¼ˆå¦‚æœé…ç½®äº†ï¼‰
                fixed_reference_str = self.config.get(
                    "TemporalFeatures", "fixed_reference_date", fallback=""
                )
                if fixed_reference_str:
                    from datetime import datetime
                    try:
                        self.fixed_reference_date = datetime.strptime(fixed_reference_str, "%Y-%m-%d")
                        logger.debug(f"å›ºå®šå‚è€ƒæ—¥æœŸé…ç½®æˆåŠŸï¼š{self.fixed_reference_date}")
                    except ValueError as ve:
                        logger.warning(f"å›ºå®šå‚è€ƒæ—¥æœŸæ ¼å¼é”™è¯¯ï¼š{fixed_reference_str}ï¼Œé”™è¯¯ï¼š{str(ve)}")
                        self.fixed_reference_date = None
                else:
                    self.fixed_reference_date = None

            except Exception as temporal_e:
                # æ—¶é—´ç‰¹å¾é…ç½®åŠ è½½å¤±è´¥æ—¶ä½¿ç”¨é»˜è®¤å€¼
                logger.warning(f"åŠ è½½æ—¶é—´ç‰¹å¾é…ç½®å¤±è´¥ï¼š{str(temporal_e)}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                self.enable_temporal_features = True
                self.date_reference = "min_date"
                self.fixed_reference_date = None

            # Step 5: éªŒè¯æ—¶é—´ç›¸å…³ç‰¹å¾æ˜¯å¦åœ¨base_numericä¸­ï¼ˆä¼˜åŒ–æ£€æŸ¥ï¼‰
            # æ³¨æ„ï¼šdays_since_workface_start å’Œ advance_distance æ˜¯åŠ¨æ€è®¡ç®—çš„ï¼Œä¸éœ€è¦åœ¨é…ç½®ä¸­
            temporal_base_fields = ['measurement_date', 'depth_from_face']

            if self.enable_temporal_features:
                # åªæ£€æŸ¥åŸºç¡€å­—æ®µæ˜¯å¦åœ¨é…ç½®ä¸­
                missing_temporal_in_config = [
                    field for field in temporal_base_fields
                    if field not in self.base_numeric
                ]

                if missing_temporal_in_config:
                    logger.warning(
                        f"å¯ç”¨äº†æ—¶é—´ç‰¹å¾ï¼Œä½†ä»¥ä¸‹åŸºç¡€æ—¶é—´å­—æ®µä¸åœ¨base_numericé…ç½®ä¸­ï¼š{missing_temporal_in_config}ã€‚"
                        f"ç³»ç»Ÿå°†åœ¨æ•°æ®é¢„å¤„ç†æ—¶è‡ªåŠ¨å¤„ç†è¿™äº›å­—æ®µã€‚"
                    )

                logger.info(f"æ—¶é—´ç‰¹å¾å¤„ç†å·²å¯ç”¨ï¼Œæ—¥æœŸå‚è€ƒåŸºå‡†ï¼š{self.date_reference}")
            else:
                logger.info("æ—¶é—´ç‰¹å¾å¤„ç†å·²ç¦ç”¨ï¼Œå°†ä»…ä½¿ç”¨ç©ºé—´å’Œå·¥ç¨‹ç‰¹å¾")

            # Step 6: æ ¡éªŒç‰¹å¾é…ç½®æœ‰æ•ˆæ€§ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            if not self.base_categorical:
                logger.warning("æœªé…ç½®åŸºç¡€åˆ†ç±»ç‰¹å¾ï¼ˆbase_categoricalï¼‰ï¼Œå¯èƒ½å½±å“æ¨¡å‹ç²¾åº¦")

            if not self.base_numeric:
                logger.warning("æœªé…ç½®åŸºç¡€æ•°å€¼ç‰¹å¾ï¼ˆbase_numericï¼‰ï¼Œæ¨¡å‹æ— æ³•è®­ç»ƒ")

            if not self.target_features:
                raise ValueError("å¿…é¡»é…ç½®è‡³å°‘ä¸€ä¸ªé¢„æµ‹ç›®æ ‡ç‰¹å¾ï¼ˆtarget_featuresï¼‰")

            # Step 7: è®°å½•åŠ è½½çš„ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
            temporal_fields_in_config = [field for field in temporal_base_fields if field in self.base_numeric]

            logger.info(
                f"ç‰¹å¾é…ç½®åŠ è½½å®Œæˆï¼š\n"
                f"  - åˆ†ç±»ç‰¹å¾ï¼š{len(self.base_categorical)}ä¸ª ({', '.join(self.base_categorical[:3])}{'...' if len(self.base_categorical) > 3 else ''})\n"
                f"  - æ•°å€¼ç‰¹å¾ï¼š{len(self.base_numeric)}ä¸ª ({', '.join(self.base_numeric[:3])}{'...' if len(self.base_numeric) > 3 else ''})\n"
                f"  - ç›®æ ‡ç‰¹å¾ï¼š{len(self.target_features)}ä¸ª ({', '.join(self.target_features)})\n"
                f"  - æ—¶é—´ç‰¹å¾ï¼š{'å·²å¯ç”¨' if self.enable_temporal_features else 'å·²ç¦ç”¨'} "
                f"(åŸºç¡€å­—æ®µï¼š{len(temporal_fields_in_config)}/{len(temporal_base_fields)})"
            )

        except Exception as e:
            logger.error(f"åŠ è½½ç‰¹å¾é…ç½®å¤±è´¥ï¼š{str(e)}", exc_info=True)

            # å¤±è´¥æ—¶è®¾ç½®é»˜è®¤å€¼ä»¥ç¡®ä¿ç³»ç»Ÿç»§ç»­è¿è¡Œ
            self.base_categorical = []
            self.base_numeric = []
            self.target_features = []
            self.enable_temporal_features = False
            self.date_reference = "min_date"
            self.fixed_reference_date = None

            logger.warning("ç‰¹å¾é…ç½®åŠ è½½å¤±è´¥ï¼Œå·²è®¾ç½®ä¸ºç©ºå€¼ï¼Œæ¨¡å‹å¯èƒ½æ— æ³•è®­ç»ƒ")

            # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…åç»­è®­ç»ƒå‡ºç°æ›´ä¸¥é‡é”™è¯¯
            raise ValueError(f"ç‰¹å¾é…ç½®åŠ è½½å¤±è´¥ï¼š{str(e)}")

    def _log_data_quality_summary(self, df):
        """ç®€å•çš„æ•°æ®è´¨é‡æ‘˜è¦æ—¥å¿—"""
        try:
            # åŸºç¡€æ£€æŸ¥
            if hasattr(self, 'target_features'):
                for target in self.target_features:
                    if target in df.columns:
                        variance = df[target].var()
                        if variance < 0.1:
                            logger.warning(f"ğŸš¨ ç›®æ ‡ç‰¹å¾ {target} æ–¹å·®è¿‡ä½: {variance:.6f}")

            # ç¼ºå¤±å€¼æ£€æŸ¥
            missing_columns = df.columns[df.isnull().any()].tolist()
            if missing_columns:
                logger.warning(f"âš ï¸ æ•°æ®åŒ…å«ç¼ºå¤±å€¼çš„åˆ—: {missing_columns}")

        except Exception as e:
            logger.debug(f"æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {str(e)}")

    def _calculate_coal_wall_emission(self, coal_thickness, tunneling_speed, initial_strength, roadway_length):
        """ç§æœ‰æ–¹æ³•ï¼šè®¡ç®—ç…¤å£ç“¦æ–¯æ¶Œå‡ºé‡ï¼ˆAQ1018â€”2006å…¬å¼ï¼‰"""
        try:
            if tunneling_speed <= 0:
                logger.warning("æ˜è¿›é€Ÿåº¦â‰¤0ï¼Œç…¤å£æ¶Œå‡ºé‡è®¾ä¸º0")
                return 0.0
            roadway_length = max(roadway_length, 0.0)
            val = coal_thickness * tunneling_speed * initial_strength * (
                    2 * np.sqrt(roadway_length / (tunneling_speed + 1e-9)) - 1
            )
            return max(0.0, float(val))
        except Exception as e:
            logger.error(f"è®¡ç®—ç…¤å£æ¶Œå‡ºé‡å¤±è´¥ï¼š{str(e)}", exc_info=True)
            return 0.0

    def _calculate_fallen_coal_emission(self, cross_section, coal_density, tunneling_speed, original_gas, residual_gas):
        """ç§æœ‰æ–¹æ³•ï¼šè®¡ç®—è½ç…¤ç“¦æ–¯æ¶Œå‡ºé‡ï¼ˆAQ1018â€”2006å…¬å¼ï¼‰"""
        try:
            gas_diff = max(0.0, (original_gas or 0.0) - (residual_gas or 0.0))
            val = cross_section * coal_density * tunneling_speed * gas_diff
            return max(0.0, float(val))
        except Exception as e:
            logger.error(f"è®¡ç®—è½ç…¤æ¶Œå‡ºé‡å¤±è´¥ï¼š{str(e)}", exc_info=True)
            return 0.0

    # ============ ä¿®æ”¹ï¼š_process_temporal_features_simpleæ–¹æ³•ä¸­çš„æ—¥æœŸå¤„ç† ============
    def _process_temporal_features_simple(self, df):
        """
        ç§æœ‰æ–¹æ³•ï¼šç®€åŒ–ç‰ˆæ—¶é—´ç‰¹å¾å¤„ç†
        åŸºäºæµ‹é‡æ—¥æœŸã€åæ ‡å’ŒéªŒè¯å­”æ·±åº¦å”¯ä¸€ç¡®å®šæµ‹é‡çŠ¶æ€

        å¤„ç†é€»è¾‘ï¼š
        1. è½¬æ¢æµ‹é‡æ—¥æœŸä¸ºdatetimeæ ¼å¼
        2. åˆ›å»ºæ—¶é—´æ•°å€¼ç‰¹å¾ï¼ˆé¿å…ç±»åˆ«ç‰¹å¾ï¼‰
        3. åˆ›å»ºæ—¶ç©ºå”¯ä¸€æ ‡è¯†

        :param df: pandas.DataFrameï¼Œè¾“å…¥æ•°æ®
        :return: pandas.DataFrameï¼Œæ·»åŠ æ—¶é—´ç‰¹å¾åçš„æ•°æ®
        """
        try:
            logger.debug("å¼€å§‹å¤„ç†æ—¶é—´ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼‰")

            # 1. ç¡®ä¿measurement_dateåˆ—å­˜åœ¨å¹¶è½¬æ¢ä¸ºdatetime
            if 'measurement_date' in df.columns:
                # è½¬æ¢æ—¥æœŸå­—ç¬¦ä¸²ä¸ºdatetime
                # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯datetimeç±»å‹
                if not pd.api.types.is_datetime64_any_dtype(df['measurement_date']):
                    df['measurement_date'] = pd.to_datetime(
                        df['measurement_date'],
                        errors='coerce',  # è½¬æ¢å¤±è´¥è®¾ä¸ºNaT
                        format='%Y-%m-%d'  # æ”¯æŒYYYY-MM-DDæ ¼å¼
                    )
                else:
                    # å¦‚æœå·²ç»æ˜¯datetimeï¼Œç¡®ä¿æ—¶åŒºä¸€è‡´
                    df['measurement_date'] = df['measurement_date'].dt.tz_localize(None)

                # 2. æ£€æŸ¥æ—¥æœŸæœ‰æ•ˆæ€§
                invalid_dates = df['measurement_date'].isna().sum()
                if invalid_dates > 0:
                    logger.warning(f"å‘ç° {invalid_dates} æ¡è®°å½•çš„æµ‹é‡æ—¥æœŸæ ¼å¼æ— æ•ˆ")
                    # å¡«å……æ— æ•ˆæ—¥æœŸä¸ºæœ€æ—©æœ‰æ•ˆæ—¥æœŸ
                    if not df['measurement_date'].isna().all():
                        min_valid_date = df['measurement_date'].min()
                        df['measurement_date'] = df['measurement_date'].fillna(min_valid_date)
                    else:
                        # å¦‚æœæ‰€æœ‰æ—¥æœŸéƒ½æ— æ•ˆï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
                        df['measurement_date'] = pd.Timestamp.now().normalize()

            else:
                logger.warning("æ•°æ®ä¸­ç¼ºå°‘measurement_dateå­—æ®µï¼Œè·³è¿‡æ—¶é—´ç‰¹å¾å¤„ç†")
                return df

            # 3. åˆ›å»ºæ—¶é—´æ•°å€¼ç‰¹å¾
            df = self._create_temporal_numeric_features(df)

            # 4. åˆ›å»ºæ—¶ç©ºå”¯ä¸€æ ‡è¯†ï¼ˆç”¨äºè¿½è¸ªåŒä¸€ä½ç½®ä¸åŒæ—¶é—´çš„æµ‹é‡ï¼‰
            df = self._create_spatiotemporal_id(df)

            logger.debug("ç®€åŒ–ç‰ˆæ—¶é—´ç‰¹å¾å¤„ç†å®Œæˆ")
            return df

        except Exception as e:
            logger.error(f"å¤„ç†æ—¶é—´ç‰¹å¾å¤±è´¥ï¼š{str(e)}ï¼Œè·³è¿‡æ—¶é—´ç‰¹å¾å¤„ç†", exc_info=True)
            return df

    # ============ 20251218æ–°å¢ï¼šè¿›å°ºç‰¹å¾å¤„ç†æ–¹æ³• ============
    def _process_mining_advance_features(self, df, is_training=True, db_utils=None):
        """
        ç§æœ‰æ–¹æ³•ï¼šå¤„ç†å›é‡‡è¿›å°ºç›¸å…³ç‰¹å¾
        æ ¸å¿ƒåŠŸèƒ½ï¼šè®¡ç®—ç´¯è®¡è¿›å°ºå’Œæœ‰æ•ˆæš´éœ²è·ç¦»

        :param df: pandas.DataFrameï¼Œè¾“å…¥æ•°æ®
        :param is_training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        :param db_utils: æ•°æ®åº“å·¥å…·å®ä¾‹ï¼Œç”¨äºé¢„æµ‹æ—¶æŸ¥è¯¢å†å²æ•°æ®
        :return: pandas.DataFrameï¼Œæ·»åŠ è¿›å°ºç‰¹å¾åçš„æ•°æ®
        """
        try:
            logger.debug(f"å¼€å§‹å¤„ç†å›é‡‡è¿›å°ºç‰¹å¾ï¼ˆæ¨¡å¼ï¼š{'è®­ç»ƒ' if is_training else 'é¢„æµ‹'}ï¼‰")

            # Step 1: ç¡®ä¿daily_advanceå­—æ®µå­˜åœ¨
            if 'daily_advance' not in df.columns:
                logger.warning("æ•°æ®ä¸­ç¼ºå°‘daily_advanceå­—æ®µï¼Œä½¿ç”¨é»˜è®¤å€¼")
                df['daily_advance'] = self.default_daily_advance
            else:
                # å¡«å……ç¼ºå¤±çš„æ—¥è¿›å°ºæ•°æ®
                daily_advance_missing = df['daily_advance'].isnull().sum()
                if daily_advance_missing > 0:
                    logger.warning(f"daily_advanceæœ‰{daily_advance_missing}æ¡ç¼ºå¤±å€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼å¡«å……")
                    df['daily_advance'] = df['daily_advance'].fillna(self.default_daily_advance)

            # Step 2: ç¡®ä¿measurement_dateå­—æ®µå­˜åœ¨ä¸”ä¸ºdatetimeæ ¼å¼
            if 'measurement_date' not in df.columns:
                logger.warning("æ•°æ®ä¸­ç¼ºå°‘measurement_dateå­—æ®µï¼Œæ— æ³•å‡†ç¡®è®¡ç®—ç´¯è®¡è¿›å°º")
                # å¦‚æœæ²¡æœ‰æ—¥æœŸï¼Œä½¿ç”¨ç®€å•ç´¯åŠ 
                df['cumulative_advance'] = df['daily_advance'].cumsum().round(2)
            else:
                # ç¡®ä¿measurement_dateä¸ºdatetimeæ ¼å¼
                if not pd.api.types.is_datetime64_any_dtype(df['measurement_date']):
                    df['measurement_date'] = pd.to_datetime(
                        df['measurement_date'], errors='coerce', format='%Y-%m-%d'
                    )

                # Step 3: æŒ‰å·¥ä½œé¢è®¡ç®—ç´¯è®¡è¿›å°º
                if 'workface_id' in df.columns:
                    # æŒ‰å·¥ä½œé¢åˆ†ç»„è®¡ç®—
                    df['cumulative_advance'] = 0.0

                    for workface_id, group in df.groupby('workface_id'):
                        # æŒ‰æµ‹é‡æ—¥æœŸæ’åº
                        group_sorted = group.sort_values('measurement_date')

                        # è®­ç»ƒæ¨¡å¼ï¼šä»0å¼€å§‹ç´¯è®¡
                        # é¢„æµ‹æ¨¡å¼ï¼šå°è¯•ä»æ•°æ®åº“è·å–å†å²ç´¯è®¡è¿›å°º
                        if is_training:
                            cumulative_advance = 0.0
                        else:
                            # é¢„æµ‹æ¨¡å¼ï¼šå°è¯•è·å–è¯¥å·¥ä½œé¢ä¸Šæ¬¡çš„ç´¯è®¡è¿›å°º
                            cumulative_advance = self._get_last_cumulative_advance(workface_id, db_utils)

                        prev_date = None

                        for idx, row in group_sorted.iterrows():
                            current_date = row['measurement_date']

                            # æ£€æŸ¥æ•°æ®æ˜¯å¦è¿ç»­ï¼ˆè®¡ç®—ä¸ä¸Šæ¬¡æµ‹é‡çš„å¤©æ•°å·®ï¼‰
                            if prev_date is not None and pd.notnull(current_date) and pd.notnull(prev_date):
                                days_diff = (current_date - prev_date).days

                                # å¦‚æœæ•°æ®ä¸­æ–­è¶…è¿‡é˜ˆå€¼ï¼Œé‡æ–°å¼€å§‹ç´¯è®¡
                                if days_diff > self.advance_data_gap_threshold:
                                    if is_training:
                                        logger.debug(f"å·¥ä½œé¢{workface_id}æ•°æ®ä¸­æ–­{days_diff}å¤©ï¼Œé‡ç½®ç´¯è®¡è¿›å°º")
                                    else:
                                        logger.warning(f"å·¥ä½œé¢{workface_id}æ•°æ®ä¸­æ–­{days_diff}å¤©ï¼Œç´¯è®¡è¿›å°ºå¯èƒ½ä¸è¿ç»­")
                                    cumulative_advance = 0.0

                            # ç´¯åŠ å½“æ—¥è¿›å°º
                            daily_advance = row['daily_advance']
                            if pd.notnull(daily_advance):
                                cumulative_advance += daily_advance

                            # æ›´æ–°ç´¯è®¡è¿›å°ºå€¼
                            df.loc[idx, 'cumulative_advance'] = round(cumulative_advance, 2)
                            prev_date = current_date

                    logger.debug(f"æŒ‰å·¥ä½œé¢è®¡ç®—ç´¯è®¡è¿›å°ºå®Œæˆï¼Œå”¯ä¸€å·¥ä½œé¢æ•°ï¼š{df['workface_id'].nunique()}")
                else:
                    # å¦‚æœæ²¡æœ‰å·¥ä½œé¢IDï¼ŒæŒ‰æ—¥æœŸæ’åºè®¡ç®—å…¨å±€ç´¯è®¡è¿›å°º
                    logger.warning("æ•°æ®ä¸­ç¼ºå°‘workface_idå­—æ®µï¼ŒæŒ‰å…¨å±€è®¡ç®—ç´¯è®¡è¿›å°º")
                    df = df.sort_values('measurement_date')
                    df['cumulative_advance'] = df['daily_advance'].cumsum().round(2)

            # Step 4: è®¡ç®—æœ‰æ•ˆæš´éœ²è·ç¦»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.enable_effective_exposure:
                if 'depth_from_face' in df.columns and 'cumulative_advance' in df.columns:
                    df['effective_exposure_distance'] = (
                            df['cumulative_advance'] + df['depth_from_face']
                    ).round(2)

                    logger.debug(
                        f"æœ‰æ•ˆæš´éœ²è·ç¦»è®¡ç®—å®Œæˆï¼ŒèŒƒå›´ï¼š[{df['effective_exposure_distance'].min():.1f}, "
                        f"{df['effective_exposure_distance'].max():.1f}]ç±³"
                    )
                else:
                    logger.warning("ç¼ºå°‘depth_from_faceæˆ–cumulative_advanceå­—æ®µï¼Œæ— æ³•è®¡ç®—æœ‰æ•ˆæš´éœ²è·ç¦»")
                    df['effective_exposure_distance'] = 0.0

            # Step 5: åˆ›å»ºè¿›å°ºç›¸å…³è¡ç”Ÿç‰¹å¾
            df = self._create_mining_advance_derived_features(df)

            logger.debug("å›é‡‡è¿›å°ºç‰¹å¾å¤„ç†å®Œæˆ")
            return df

        except Exception as e:
            logger.error(f"å¤„ç†å›é‡‡è¿›å°ºç‰¹å¾å¤±è´¥ï¼š{str(e)}ï¼Œè·³è¿‡æ­¤æ­¥éª¤", exc_info=True)
            # å¤±è´¥æ—¶æ·»åŠ é»˜è®¤å€¼ï¼Œç¡®ä¿æµç¨‹ç»§ç»­
            df['cumulative_advance'] = df.get('daily_advance', self.default_daily_advance)
            df['effective_exposure_distance'] = df.get('depth_from_face', 0.0)
            return df

    def _get_last_cumulative_advance(self, workface_id, db_utils):
        """
        è·å–å·¥ä½œé¢ä¸Šæ¬¡çš„ç´¯è®¡è¿›å°ºï¼ˆç”¨äºé¢„æµ‹æ¨¡å¼ï¼‰

        :param workface_id: å·¥ä½œé¢ID
        :param db_utils: æ•°æ®åº“å·¥å…·å®ä¾‹
        :return: ä¸Šæ¬¡ç´¯è®¡è¿›å°ºå€¼ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›0
        """
        if db_utils is None:
            logger.warning("æœªæä¾›æ•°æ®åº“å·¥å…·ï¼Œæ— æ³•æŸ¥è¯¢å†å²ç´¯è®¡è¿›å°º")
            return 0.0

        try:
            # ä»æ•°æ®åº“æŸ¥è¯¢è¯¥å·¥ä½œé¢ä¸Šæ¬¡è®­ç»ƒæ—¶çš„ç´¯è®¡è¿›å°º
            # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾æ•°æ®åº“ä¸­å·²ä¿å­˜äº†è®­ç»ƒæ—¶çš„ç´¯è®¡è¿›å°º
            db_conf = db_utils.db_config
            db_url = f"mysql+pymysql://{db_conf['user']}:{db_conf['password']}@" \
                     f"{db_conf['host']}:{db_conf['port']}/{db_conf['db']}?charset={db_conf['charset']}"
            engine = create_engine(db_url)

            with engine.connect() as conn:
                from sqlalchemy import text
                # æŸ¥è¯¢è¯¥å·¥ä½œé¢çš„æœ€å¤§ç´¯è®¡è¿›å°º
                query = text("""
                    SELECT MAX(cumulative_advance) as last_advance
                    FROM t_prediction_parameters 
                    WHERE workface_id = :workface_id 
                      AND cumulative_advance IS NOT NULL
                """)

                result = conn.execute(query, {"workface_id": workface_id})
                row = result.fetchone()

                if row and row[0] is not None:
                    last_advance = float(row[0])
                    logger.debug(f"æŸ¥è¯¢åˆ°å·¥ä½œé¢{workface_id}çš„å†å²ç´¯è®¡è¿›å°ºï¼š{last_advance}")
                    return last_advance
                else:
                    logger.warning(f"å·¥ä½œé¢{workface_id}æ— å†å²ç´¯è®¡è¿›å°ºè®°å½•")
                    return 0.0

        except Exception as e:
            logger.error(f"æŸ¥è¯¢å†å²ç´¯è®¡è¿›å°ºå¤±è´¥ï¼š{str(e)}")
            return 0.0

    def _create_mining_advance_derived_features(self, df):
        """
        ç§æœ‰æ–¹æ³•ï¼šåˆ›å»ºè¿›å°ºç›¸å…³è¡ç”Ÿç‰¹å¾

        :param df: pandas.DataFrameï¼Œè¾“å…¥æ•°æ®
        :return: pandas.DataFrameï¼Œæ·»åŠ è¡ç”Ÿç‰¹å¾åçš„æ•°æ®
        """
        try:
            # 1. è¿›å°ºå˜åŒ–ç‡ï¼ˆå½“æ—¥è¿›å°º/ç´¯è®¡è¿›å°ºï¼Œé¿å…é™¤ä»¥é›¶ï¼‰
            if 'cumulative_advance' in df.columns and 'daily_advance' in df.columns:
                df['advance_change_rate'] = df.apply(
                    lambda row: (
                        row['daily_advance'] / row['cumulative_advance']
                        if row['cumulative_advance'] > 0
                        else 0.0
                    ),
                    axis=1
                ).round(4)

            # 2. è¿›å°ºé˜¶æ®µåˆ†ç»„ï¼ˆæ¯20ç±³ä¸€ä¸ªé˜¶æ®µï¼‰
            if 'cumulative_advance' in df.columns:
                # åˆ›å»ºè¿›å°ºé˜¶æ®µ
                max_advance = df['cumulative_advance'].max()
                num_stages = max(1, int(max_advance / 20) + 1)
                bins = [i * 20 for i in range(num_stages + 1)]
                labels = [f"é˜¶æ®µ{i + 1}({bins[i]}-{bins[i + 1]}ç±³)" for i in range(num_stages)]

                df['advance_stage'] = pd.cut(
                    df['cumulative_advance'],
                    bins=bins,
                    labels=labels[:num_stages],
                    right=False
                )

                logger.debug(f"è¿›å°ºé˜¶æ®µåˆ†ç»„å®Œæˆï¼Œå…±{num_stages}ä¸ªé˜¶æ®µ")

            # 3. è¿›å°ºé€Ÿåº¦ç±»åˆ«
            if 'daily_advance' in df.columns:
                # ä½¿ç”¨åˆ†ä½æ•°ç¡®å®šé€Ÿåº¦ç±»åˆ«é˜ˆå€¼
                q33 = df['daily_advance'].quantile(0.33) if len(df) > 0 else 2.0
                q66 = df['daily_advance'].quantile(0.66) if len(df) > 0 else 4.0

                def classify_speed(val):
                    if val <= q33:
                        return "æ…¢é€Ÿ"
                    elif val <= q66:
                        return "ä¸­é€Ÿ"
                    else:
                        return "å¿«é€Ÿ"

                df['advance_speed_category'] = df['daily_advance'].apply(classify_speed)

                # ç»Ÿè®¡å„é€Ÿåº¦ç±»åˆ«æ•°é‡
                speed_counts = df['advance_speed_category'].value_counts()
                logger.debug(f"è¿›å°ºé€Ÿåº¦åˆ†ç±»å®Œæˆï¼š{dict(speed_counts)}")

            return df

        except Exception as e:
            logger.warning(f"åˆ›å»ºè¿›å°ºè¡ç”Ÿç‰¹å¾å¤±è´¥ï¼š{str(e)}ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
            return df

    # ============ 20251218 æ–°å¢ç»“æŸ ============

    def _create_temporal_numeric_features(self, df):
        """
        ç§æœ‰æ–¹æ³•ï¼šåˆ›å»ºæ—¶é—´æ•°å€¼ç‰¹å¾

        :param df: pandas.DataFrameï¼Œè¾“å…¥æ•°æ®
        :return: pandas.DataFrameï¼Œæ·»åŠ æ—¶é—´æ•°å€¼ç‰¹å¾åçš„æ•°æ®
        """
        try:
            if 'measurement_date' in df.columns and not df['measurement_date'].isna().all():
                # ç¡®ä¿measurement_dateæ˜¯datetimeç±»å‹
                if not pd.api.types.is_datetime64_any_dtype(df['measurement_date']):
                    df['measurement_date'] = pd.to_datetime(df['measurement_date'], errors='coerce')

                # ç¡®å®šå‚è€ƒæ—¥æœŸ
                if self.date_reference == "fixed_date" and self.fixed_reference_date:
                    reference_date = self.fixed_reference_date
                else:
                    # ä½¿ç”¨æœ€æ—©æµ‹é‡æ—¥æœŸä½œä¸ºå‚è€ƒ
                    reference_date = df['measurement_date'].min()
                    logger.debug(f"ä½¿ç”¨æœ€æ—©æµ‹é‡æ—¥æœŸä½œä¸ºå‚è€ƒæ—¥æœŸ: {reference_date}")

                # ç¡®ä¿reference_dateæ˜¯datetime
                if isinstance(reference_date, pd.Timestamp):
                    reference_date = reference_date.to_pydatetime()

                # è®¡ç®—ä»å‚è€ƒæ—¥æœŸå¼€å§‹çš„å¤©æ•°
                df['days_since_reference'] = (df['measurement_date'] - reference_date).dt.days

                # ç¡®ä¿å¤©æ•°ä¸ºéè´Ÿï¼ˆå¯¹äºå‚è€ƒæ—¥æœŸä¹‹å‰çš„æ—¥æœŸï¼‰
                df['days_since_reference'] = df['days_since_reference'].clip(lower=0)

                # æå–æ—¥æœŸç»„ä»¶ï¼ˆå¯é€‰ï¼‰ - è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                df['day_of_week'] = df['measurement_date'].dt.dayofweek.astype(float)  # å‘¨å‡ ï¼ˆ0=å‘¨ä¸€ï¼‰
                df['day_of_month'] = df['measurement_date'].dt.day.astype(float)  # æœˆä¸­çš„ç¬¬å‡ å¤©
                df['month'] = df['measurement_date'].dt.month.astype(float)  # æœˆä»½

                # ç¡®ä¿æ²¡æœ‰NaNå€¼
                date_features = ['days_since_reference', 'day_of_week', 'day_of_month', 'month']
                for feat in date_features:
                    if feat in df.columns:
                        df[feat] = df[feat].fillna(0).astype(float)

                logger.debug(
                    f"åˆ›å»ºæ—¶é—´æ•°å€¼ç‰¹å¾å®Œæˆï¼šå‚è€ƒæ—¥æœŸ={reference_date}ï¼Œæ—¥æœŸèŒƒå›´={df['measurement_date'].min()}åˆ°{df['measurement_date'].max()}"
                )

            return df
        except Exception as e:
            logger.warning(f"åˆ›å»ºæ—¶é—´æ•°å€¼ç‰¹å¾å¤±è´¥ï¼š{str(e)}ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
            return df

    def _create_spatiotemporal_id(self, df):
        """
        ç§æœ‰æ–¹æ³•ï¼šåˆ›å»ºæ—¶ç©ºå”¯ä¸€æ ‡è¯†
        æ ¼å¼ï¼šåæ ‡_æ—¥æœŸ_æ·±åº¦ï¼Œç”¨äºå”¯ä¸€æ ‡è¯†ä¸€ä¸ªæµ‹é‡çŠ¶æ€

        :param df: pandas.DataFrameï¼Œè¾“å…¥æ•°æ®
        :return: pandas.DataFrameï¼Œæ·»åŠ æ—¶ç©ºæ ‡è¯†åçš„æ•°æ®
        """
        try:
            # æ£€æŸ¥å¿…éœ€çš„å­—æ®µ
            required_fields = ['x_coord', 'y_coord', 'z_coord', 'measurement_date', 'depth_from_face']
            missing_fields = [field for field in required_fields if field not in df.columns]

            if missing_fields:
                logger.warning(f"æ— æ³•åˆ›å»ºæ—¶ç©ºæ ‡è¯†ï¼Œç¼ºå°‘å­—æ®µï¼š{missing_fields}")
                return df

            # åˆ›å»ºæ—¶ç©ºå”¯ä¸€æ ‡è¯†
            df['spatiotemporal_id'] = (
                    df['x_coord'].round(1).astype(str) + '_' +
                    df['y_coord'].round(1).astype(str) + '_' +
                    df['z_coord'].round(1).astype(str) + '_' +
                    df['measurement_date'].dt.strftime('%Y%m%d') + '_' +
                    df['depth_from_face'].round(1).astype(str)
            )

            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„æ—¶ç©ºæ ‡è¯†ï¼ˆä¸åº”è¯¥æœ‰ï¼‰
            duplicate_count = df['spatiotemporal_id'].duplicated().sum()
            if duplicate_count > 0:
                logger.warning(f"å‘ç° {duplicate_count} ä¸ªé‡å¤çš„æ—¶ç©ºæ ‡è¯†ï¼Œå¯èƒ½å­˜åœ¨é‡å¤æ•°æ®")

            logger.debug(f"åˆ›å»ºæ—¶ç©ºæ ‡è¯†å®Œæˆï¼Œå”¯ä¸€æ ‡è¯†æ•°é‡ï¼š{df['spatiotemporal_id'].nunique()}")
            return df
        except Exception as e:
            logger.warning(f"åˆ›å»ºæ—¶ç©ºæ ‡è¯†å¤±è´¥ï¼š{str(e)}ï¼Œè·³è¿‡æ­¤æ­¥éª¤")
            return df


# ============ 20251218 æ–°å¢ï¼šæ—¶ç©ºç‰¹å¾æå–å™¨ç±» ============
class SpatiotemporalFeatureExtractor:
    """
    æ—¶ç©ºç‰¹å¾æå–å™¨
    è´Ÿè´£ä»åŸå§‹æ•°æ®ä¸­æå–æ—¶ç©ºç›¸å…³ç‰¹å¾ï¼Œå¢å¼ºæ¨¡å‹å¯¹æ—¶ç©ºæ¨¡å¼çš„å­¦ä¹ èƒ½åŠ›
    """

    def __init__(self, config_path="config.ini"):
        """
        åˆå§‹åŒ–æ—¶ç©ºç‰¹å¾æå–å™¨
        :param config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        from loguru import logger
        self.logger = logger

        # åŠ è½½é…ç½®
        import configparser
        self.config = configparser.ConfigParser()
        self.config.read(config_path, encoding="utf-8")

        # æ—¶ç©ºç‰¹å¾ç›¸å…³å‚æ•°
        self.spatial_radius = self.config.getfloat("SpatialFeatures", "spatial_radius", fallback=15.0)
        self.temporal_lag_days = self.config.getint("TemporalFeatures", "temporal_lag_days", fallback=5)
        self.enable_spatial_features = self.config.getboolean("SpatialFeatures", "enable_spatial_features",
                                                              fallback=True)
        self.enable_temporal_features = self.config.getboolean("TemporalFeatures", "enable_temporal_features",
                                                               fallback=True)

        self.logger.info(
            f"æ—¶ç©ºç‰¹å¾æå–å™¨åˆå§‹åŒ–å®Œæˆ: ç©ºé—´åŠå¾„={self.spatial_radius}m, æ—¶é—´æ»å={self.temporal_lag_days}å¤©")

    def extract_features(self, df, is_training=True):
        """
        æå–æ—¶ç©ºç‰¹å¾
        :param df: pandas.DataFrameï¼Œé¢„å¤„ç†åçš„æ•°æ®
        :param is_training: boolï¼Œæ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        :return: pandas.DataFrameï¼ŒåŒ…å«æ–°ç‰¹å¾çš„æ•°æ®
        """
        try:
            self.logger.debug("å¼€å§‹æå–æ—¶ç©ºç‰¹å¾")

            # åˆ›å»ºæ•°æ®å‰¯æœ¬
            df_enhanced = df.copy()

            # 1. æå–ç©ºé—´ç‰¹å¾
            if self.enable_spatial_features:
                df_enhanced = self._extract_spatial_features(df_enhanced)

            # 2. æå–æ—¶é—´åºåˆ—ç‰¹å¾
            if self.enable_temporal_features:
                df_enhanced = self._extract_temporal_features(df_enhanced)

            # 3. æå–è¡°å‡è§„å¾‹ç‰¹å¾
            df_enhanced = self._extract_decay_features(df_enhanced)

            # 4. æå–äº¤äº’ç‰¹å¾
            df_enhanced = self._extract_interaction_features(df_enhanced)

            self.logger.debug(f"æ—¶ç©ºç‰¹å¾æå–å®Œæˆï¼Œæ–°å¢ç‰¹å¾æ•°: {len(df_enhanced.columns) - len(df.columns)}")

            # è®°å½•æ–°å¢çš„ç‰¹å¾å
            original_columns = set(df.columns)
            enhanced_columns = set(df_enhanced.columns)
            new_features = list(enhanced_columns - original_columns)

            # å­˜å‚¨æ–°å¢ç‰¹å¾åï¼Œä¾›å¤–éƒ¨è®¿é—®
            self.new_feature_names = new_features

            return df_enhanced

        except Exception as e:
            self.logger.error(f"æå–æ—¶ç©ºç‰¹å¾å¤±è´¥: {str(e)}", exc_info=True)
            # å¤±è´¥æ—¶è¿”å›åŸå§‹æ•°æ®
            return df

    def _extract_spatial_features(self, df):
        """
        æå–ç©ºé—´ç‰¹å¾
        :param df: pandas.DataFrame
        :return: pandas.DataFrameï¼ŒåŒ…å«ç©ºé—´ç‰¹å¾
        """
        try:
            self.logger.debug("å¼€å§‹æå–ç©ºé—´ç‰¹å¾")

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_cols = ['x_coord', 'y_coord', 'z_coord', 'measurement_date']
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                self.logger.warning(f"ç¼ºå°‘ç©ºé—´ç‰¹å¾æ‰€éœ€å­—æ®µ: {missing_cols}ï¼Œè·³è¿‡ç©ºé—´ç‰¹å¾æå–")
                return df

            # 1. åˆ›å»ºä½ç½®æ ‡è¯†ï¼ˆå››èˆäº”å…¥åˆ°1ç±³ç²¾åº¦ï¼‰
            df['position_key'] = (
                    df['x_coord'].round(1).astype(str) + '_' +
                    df['y_coord'].round(1).astype(str) + '_' +
                    df['z_coord'].round(1).astype(str)
            )

            # 2. æŒ‰æ—¥æœŸåˆ†ç»„ï¼Œè®¡ç®—ç©ºé—´é‚»å±…ç‰¹å¾
            from scipy.spatial import KDTree
            import numpy as np

            # å­˜å‚¨ç©ºé—´ç‰¹å¾
            spatial_features = []

            for date, group in df.groupby('measurement_date'):
                if len(group) < 2:
                    # å•ä¸ªç‚¹æ— æ³•è®¡ç®—ç©ºé—´é‚»å±…
                    continue

                # æå–åæ ‡
                coords = group[['x_coord', 'y_coord', 'z_coord']].values

                # æ„å»ºKDæ ‘è¿›è¡Œå¿«é€Ÿç©ºé—´æœç´¢
                tree = KDTree(coords)

                # ä¸ºæ¯ä¸ªç‚¹æŸ¥æ‰¾é‚»å±…
                for i, (idx, row) in enumerate(group.iterrows()):
                    # æŸ¥æ‰¾åŠå¾„å†…çš„é‚»å±…ï¼ˆä¸åŒ…æ‹¬è‡ªå·±ï¼‰
                    neighbors = tree.query_ball_point(coords[i], self.spatial_radius)
                    neighbors = [n for n in neighbors if n != i]  # æ’é™¤è‡ªèº«

                    if neighbors:
                        neighbor_data = group.iloc[neighbors]

                        # è®¡ç®—é‚»å±…ç»Ÿè®¡ç‰¹å¾
                        row_data = row.copy()

                        # é‚»å±…ç“¦æ–¯å‚æ•°ç»Ÿè®¡
                        if 'gas_emission_q' in neighbor_data.columns:
                            row_data['neighbor_q_mean'] = neighbor_data['gas_emission_q'].mean()
                            row_data['neighbor_q_std'] = neighbor_data['gas_emission_q'].std()
                            row_data['neighbor_q_min'] = neighbor_data['gas_emission_q'].min()
                            row_data['neighbor_q_max'] = neighbor_data['gas_emission_q'].max()

                        # é‚»å±…é’»å±‘é‡ç»Ÿè®¡
                        if 'drilling_cuttings_s' in neighbor_data.columns:
                            row_data['neighbor_s_mean'] = neighbor_data['drilling_cuttings_s'].mean()
                            row_data['neighbor_s_std'] = neighbor_data['drilling_cuttings_s'].std()

                        # é‚»å±…æ·±åº¦ç»Ÿè®¡
                        if 'depth_from_face' in neighbor_data.columns:
                            row_data['neighbor_depth_mean'] = neighbor_data['depth_from_face'].mean()

                        # é‚»å±…æ•°é‡
                        row_data['neighbor_count'] = len(neighbors)

                        # æœ€è¿‘é‚»è·ç¦»
                        if len(neighbors) > 0:
                            distances = np.linalg.norm(coords[i] - coords[neighbors], axis=1)
                            row_data['nearest_neighbor_distance'] = distances.min()
                            row_data['mean_neighbor_distance'] = distances.mean()

                        spatial_features.append(row_data)
                    else:
                        # æ²¡æœ‰é‚»å±…çš„æƒ…å†µ
                        row_data = row.copy()
                        row_data['neighbor_count'] = 0
                        row_data['nearest_neighbor_distance'] = np.nan
                        spatial_features.append(row_data)

            if spatial_features:
                # åˆå¹¶ç©ºé—´ç‰¹å¾
                spatial_df = pd.DataFrame(spatial_features)

                # å°†ç©ºé—´ç‰¹å¾åˆå¹¶å›åŸæ•°æ®
                spatial_cols = [col for col in spatial_df.columns if
                                col.startswith('neighbor_') or col in ['nearest_neighbor_distance',
                                                                       'mean_neighbor_distance']]
                spatial_cols.append('position_key')

                for col in spatial_cols:
                    if col in spatial_df.columns:
                        df[col] = spatial_df[col]

            self.logger.debug(
                f"ç©ºé—´ç‰¹å¾æå–å®Œæˆï¼Œæ–°å¢ {len([c for c in df.columns if c.startswith('neighbor_')])} ä¸ªç‰¹å¾")

            return df

        except Exception as e:
            self.logger.error(f"æå–ç©ºé—´ç‰¹å¾å¤±è´¥: {str(e)}", exc_info=True)
            return df

    def _extract_temporal_features(self, df):
        """
        æå–æ—¶é—´åºåˆ—ç‰¹å¾
        :param df: pandas.DataFrame
        :return: pandas.DataFrameï¼ŒåŒ…å«æ—¶é—´ç‰¹å¾
        """
        global temporal_cols
        try:
            self.logger.debug("å¼€å§‹æå–æ—¶é—´åºåˆ—ç‰¹å¾")

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if 'position_key' not in df.columns:
                self.logger.warning("ç¼ºå°‘position_keyå­—æ®µï¼Œæ— æ³•æå–æ—¶é—´åºåˆ—ç‰¹å¾")
                return df

            if 'measurement_date' not in df.columns:
                self.logger.warning("ç¼ºå°‘measurement_dateå­—æ®µï¼Œæ— æ³•æå–æ—¶é—´åºåˆ—ç‰¹å¾")
                return df

            # ç¡®ä¿æ—¥æœŸæ ¼å¼
            if not pd.api.types.is_datetime64_any_dtype(df['measurement_date']):
                df['measurement_date'] = pd.to_datetime(df['measurement_date'], errors='coerce')

            # æŒ‰ä½ç½®åˆ†ç»„
            temporal_features = []

            for pos_key, group in df.groupby('position_key'):
                if len(group) < 2:
                    # å•ä¸ªæ—¶é—´ç‚¹æ— æ³•è®¡ç®—æ—¶é—´åºåˆ—ç‰¹å¾
                    continue

                # æŒ‰æ—¶é—´æ’åº
                group_sorted = group.sort_values('measurement_date').copy()

                # è®¡ç®—æ—¶é—´åºåˆ—ç‰¹å¾
                for i, (idx, row) in enumerate(group_sorted.iterrows()):
                    row_data = row.copy()

                    # 1. æµ‹é‡é¡ºåº
                    row_data['measurement_order'] = i + 1

                    # 2. è·ç¦»é¦–æ¬¡æµ‹é‡çš„å¤©æ•°
                    if i == 0:
                        row_data['days_since_first_measure'] = 0
                    else:
                        first_date = group_sorted.iloc[0]['measurement_date']
                        row_data['days_since_first_measure'] = (row['measurement_date'] - first_date).days

                    # 3. è·ç¦»ä¸Šæ¬¡æµ‹é‡çš„å¤©æ•°
                    if i > 0:
                        prev_date = group_sorted.iloc[i - 1]['measurement_date']
                        row_data['days_since_last_measure'] = (row['measurement_date'] - prev_date).days
                    else:
                        row_data['days_since_last_measure'] = np.nan

                    # 4. ç´¯è®¡æµ‹é‡æ¬¡æ•°
                    row_data['cumulative_measure_count'] = i + 1

                    # 5. è®¡ç®—å˜åŒ–ç‡ï¼ˆå¦‚æœæœ‰å‰ä¸€æ¬¡æµ‹é‡ï¼‰
                    if i > 0:
                        if 'gas_emission_q' in group_sorted.columns:
                            prev_q = group_sorted.iloc[i - 1]['gas_emission_q']
                            if prev_q != 0:
                                row_data['q_change_rate'] = (row['gas_emission_q'] - prev_q) / prev_q
                            else:
                                row_data['q_change_rate'] = np.nan

                        if 'drilling_cuttings_s' in group_sorted.columns:
                            prev_s = group_sorted.iloc[i - 1]['drilling_cuttings_s']
                            if prev_s != 0:
                                row_data['s_change_rate'] = (row['drilling_cuttings_s'] - prev_s) / prev_s
                            else:
                                row_data['s_change_rate'] = np.nan

                    temporal_features.append(row_data)

            if temporal_features:
                # åˆå¹¶æ—¶é—´ç‰¹å¾
                temporal_df = pd.DataFrame(temporal_features)

                # å°†æ—¶é—´ç‰¹å¾åˆå¹¶å›åŸæ•°æ®
                temporal_cols = ['measurement_order', 'days_since_first_measure',
                                 'days_since_last_measure', 'cumulative_measure_count',
                                 'q_change_rate', 's_change_rate']

                for col in temporal_cols:
                    if col in temporal_df.columns:
                        # åˆå¹¶æ—¶æ³¨æ„ç´¢å¼•å¯¹é½
                        if col in df.columns:
                            df[col] = temporal_df[col].combine_first(df[col])
                        else:
                            df[col] = temporal_df[col]

            self.logger.debug(f"æ—¶é—´åºåˆ—ç‰¹å¾æå–å®Œæˆï¼Œæ–°å¢ {len([c for c in temporal_cols if c in df.columns])} ä¸ªç‰¹å¾")

            return df

        except Exception as e:
            self.logger.error(f"æå–æ—¶é—´åºåˆ—ç‰¹å¾å¤±è´¥: {str(e)}", exc_info=True)
            return df

    def _extract_decay_features(self, df):
        """
        æå–è¡°å‡è§„å¾‹ç‰¹å¾
        :param df: pandas.DataFrame
        :return: pandas.DataFrameï¼ŒåŒ…å«è¡°å‡ç‰¹å¾
        """
        try:
            self.logger.debug("å¼€å§‹æå–è¡°å‡è§„å¾‹ç‰¹å¾")

            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_cols = ['depth_from_face']
            if 'cumulative_advance' in df.columns:
                required_cols.append('cumulative_advance')
            if 'days_since_first_measure' in df.columns:
                required_cols.append('days_since_first_measure')

            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                self.logger.debug(f"ç¼ºå°‘è¡°å‡ç‰¹å¾æ‰€éœ€å­—æ®µ: {missing_cols}ï¼Œä½¿ç”¨ç®€åŒ–è¡°å‡æ¨¡å‹")

            # 1. æ·±åº¦è¡°å‡å› å­ï¼ˆå‡è®¾æŒ‡æ•°è¡°å‡ï¼‰
            if 'depth_from_face' in df.columns:
                # æ·±åº¦è¡°å‡ç³»æ•°ï¼ˆç»éªŒå€¼ï¼šæ¯ç±³è¡°å‡5%ï¼‰
                depth_decay_coef = 0.05
                df['depth_decay_factor'] = np.exp(-depth_decay_coef * df['depth_from_face'])

            # 2. æ—¶é—´è¡°å‡å› å­ï¼ˆå¦‚æœæœ‰æ—¶é—´ä¿¡æ¯ï¼‰
            if 'days_since_first_measure' in df.columns:
                # æ—¶é—´è¡°å‡ç³»æ•°ï¼ˆç»éªŒå€¼ï¼šæ¯å¤©è¡°å‡1%ï¼‰
                time_decay_coef = 0.01
                df['time_decay_factor'] = np.exp(-time_decay_coef * df['days_since_first_measure'])

            # 3. ç»¼åˆè¡°å‡å› å­
            decay_cols = [c for c in df.columns if 'decay_factor' in c]
            if len(decay_cols) > 0:
                df['total_decay_factor'] = df[decay_cols].product(axis=1)

                # 4. è®¡ç®—è¡°å‡è°ƒæ•´åçš„ç“¦æ–¯å€¼
                if 'gas_emission_q' in df.columns:
                    df['q_decay_adjusted'] = df['gas_emission_q'] / df['total_decay_factor'].clip(lower=0.001)

                if 'drilling_cuttings_s' in df.columns:
                    df['s_decay_adjusted'] = df['drilling_cuttings_s'] / df['total_decay_factor'].clip(lower=0.001)

            # 5. æ·±åº¦åˆ†ç»„ç‰¹å¾
            if 'depth_from_face' in df.columns:
                # åˆ›å»ºæ·±åº¦åˆ†ç»„ï¼ˆ0-2m, 2-4m, 4-6m, 6-8m, 8-10m, 10m+ï¼‰
                bins = [0, 2, 4, 6, 8, 10, float('inf')]
                labels = ['æ·±åº¦0-2m', 'æ·±åº¦2-4m', 'æ·±åº¦4-6m', 'æ·±åº¦6-8m', 'æ·±åº¦8-10m', 'æ·±åº¦10mä»¥ä¸Š']
                df['depth_group'] = pd.cut(df['depth_from_face'], bins=bins, labels=labels, right=False)

            self.logger.debug(
                f"è¡°å‡è§„å¾‹ç‰¹å¾æå–å®Œæˆï¼Œæ–°å¢ {len([c for c in df.columns if 'decay' in c or 'depth_group' in c])} ä¸ªç‰¹å¾")

            return df

        except Exception as e:
            self.logger.error(f"æå–è¡°å‡è§„å¾‹ç‰¹å¾å¤±è´¥: {str(e)}", exc_info=True)
            return df

    def _extract_interaction_features(self, df):
        """
        æå–äº¤äº’ç‰¹å¾ï¼ˆç©ºé—´Ã—æ—¶é—´äº¤äº’ï¼‰
        :param df: pandas.DataFrame
        :return: pandas.DataFrameï¼ŒåŒ…å«äº¤äº’ç‰¹å¾
        """
        try:
            self.logger.debug("å¼€å§‹æå–äº¤äº’ç‰¹å¾")

            # 1. ç©ºé—´é‚»å±…ä¸æ—¶é—´å˜åŒ–çš„äº¤äº’
            if 'neighbor_q_mean' in df.columns and 'q_change_rate' in df.columns:
                df['neighbor_q_vs_change'] = df['neighbor_q_mean'] * (1 + df['q_change_rate'].fillna(0))

            # 2. æ·±åº¦ä¸æ—¶é—´çš„äº¤äº’
            if 'depth_from_face' in df.columns and 'days_since_first_measure' in df.columns:
                df['depth_time_interaction'] = df['depth_from_face'] * df['days_since_first_measure']

            # 3. ç´¯è®¡è¿›å°ºä¸æ·±åº¦çš„äº¤äº’
            if 'cumulative_advance' in df.columns and 'depth_from_face' in df.columns:
                df['advance_depth_interaction'] = df['cumulative_advance'] * df['depth_from_face']

            self.logger.debug(
                f"äº¤äº’ç‰¹å¾æå–å®Œæˆï¼Œæ–°å¢ {len([c for c in df.columns if 'interaction' in c or '_vs_' in c])} ä¸ªç‰¹å¾")

            return df

        except Exception as e:
            self.logger.error(f"æå–äº¤äº’ç‰¹å¾å¤±è´¥: {str(e)}", exc_info=True)
            return df

    def get_new_feature_names(self):
        """
        è·å–æ–°å¢çš„ç‰¹å¾å
        :return: listï¼Œæ–°å¢ç‰¹å¾ååˆ—è¡¨
        """
        return getattr(self, 'new_feature_names', [])

    def get_all_new_feature_categories(self):
        """
        è·å–æ‰€æœ‰æ–°å¢ç‰¹å¾çš„åˆ†ç±»
        :return: dictï¼ŒæŒ‰ç±»åˆ«åˆ†ç»„çš„æ–°å¢ç‰¹å¾
        """
        if not hasattr(self, 'new_feature_names'):
            return {}

        categories = {
            'spatial_features': [],
            'temporal_features': [],
            'decay_features': [],
            'interaction_features': [],
            'other_features': []
        }

        for feature in self.new_feature_names:
            if feature.startswith('neighbor_'):
                categories['spatial_features'].append(feature)
            elif any(keyword in feature for keyword in ['change', 'measure', 'order', 'since']):
                categories['temporal_features'].append(feature)
            elif any(keyword in feature for keyword in ['decay', 'adjusted', 'depth_group']):
                categories['decay_features'].append(feature)
            elif any(keyword in feature for keyword in ['interaction', '_vs_']):
                categories['interaction_features'].append(feature)
            else:
                categories['other_features'].append(feature)

        return categories
# ============ 20251218æ–°å¢ï¼šæ—¶ç©ºç‰¹å¾æå–å™¨ç±»ç»“æŸ ============