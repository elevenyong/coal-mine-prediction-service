"""
ç…¤çŸ¿ç“¦æ–¯é£é™©é¢„æµ‹ç³»ç»Ÿ - æ•°æ®é¢„å¤„ç†æ¨¡å—
åŒ…å«ï¼šæ•°æ®é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹ã€åˆ†æºç‰¹å¾è®¡ç®—
"""
import pandas as pd
import numpy as np
from loguru import logger

from config_utils import ConfigUtils


class DataPreprocessor(ConfigUtils):
    """æ•°æ®é¢„å¤„ç†å™¨"""

    def __init__(self, config_path="config.ini"):
        super().__init__(config_path)
        self._load_feature_config()  # åœ¨åˆå§‹åŒ–æ—¶åŠ è½½ç‰¹å¾é…ç½®

        # æ·»åŠ ç¼ºå¤±çš„å±æ€§
        self.is_trained = False
        self.training_features = None

    def _load_feature_config(self):
        """
        ç§æœ‰æ–¹æ³•ï¼šåŠ è½½ç‰¹å¾é…ç½®ï¼ˆ[Features] sectionï¼‰
        æ³¨æ„ï¼šå·²ç§»é™¤ç“¦æ–¯æ¶Œå‡ºé‡ç›¸å…³ç‰¹å¾
        """
        try:
            # Step 1: è¯»å–åˆ†ç±»ç‰¹å¾
            categorical_str = self.config.get("Features", "base_categorical", fallback="")
            self.base_categorical = [x.strip() for x in categorical_str.split(",") if x.strip()]
            # Step 2: è¯»å–æ•°å€¼ç‰¹å¾ï¼ˆå·²ç§»é™¤åˆ†æºé¢„æµ‹æ³•å‚æ•°ï¼‰
            numeric_str = self.config.get("Features", "base_numeric", fallback="")
            self.base_numeric = [x.strip() for x in numeric_str.split(",") if x.strip()]
            # Step 3: è¯»å–é¢„æµ‹ç›®æ ‡ç‰¹å¾ï¼ˆåªä¿ç•™é’»å±‘é‡å’Œç“¦æ–¯æ¶Œå‡ºé€Ÿåº¦ï¼‰
            target_str = self.config.get("Features", "target_features", fallback="")
            self.target_features = [x.strip() for x in target_str.split(",") if x.strip()]
            # æ ¡éªŒç‰¹å¾é…ç½®æœ‰æ•ˆæ€§
            if not self.base_categorical:
                logger.warning("æœªé…ç½®åŸºç¡€åˆ†ç±»ç‰¹å¾ï¼ˆbase_categoricalï¼‰ï¼Œå¯èƒ½å½±å“æ¨¡å‹ç²¾åº¦")
            if not self.base_numeric:
                logger.warning("æœªé…ç½®åŸºç¡€æ•°å€¼ç‰¹å¾ï¼ˆbase_numericï¼‰ï¼Œæ¨¡å‹æ— æ³•è®­ç»ƒ")
            if not self.target_features:
                raise ValueError("å¿…é¡»é…ç½®è‡³å°‘ä¸€ä¸ªé¢„æµ‹ç›®æ ‡ç‰¹å¾ï¼ˆtarget_featuresï¼‰")
            logger.debug(
                f"ç‰¹å¾é…ç½®åŠ è½½å®Œæˆï¼ˆå·²ç§»é™¤ç“¦æ–¯æ¶Œå‡ºé‡ç›¸å…³ç‰¹å¾ï¼‰ï¼š"
                f"åˆ†ç±»ç‰¹å¾ï¼š{self.base_categorical}ï¼Œ"
                f"æ•°å€¼ç‰¹å¾ï¼š{self.base_numeric}ï¼Œ"
                f"ç›®æ ‡ç‰¹å¾ï¼š{self.target_features}"
            )
        except Exception as e:
            logger.error(f"åŠ è½½ç‰¹å¾é…ç½®å¤±è´¥ï¼š{str(e)}", exc_info=True)
            raise

    def preprocess_data(self, data, is_training=True, fault_calculator=None, db_utils=None):
        """
        å…¬å¼€æ–¹æ³•ï¼šæ•°æ®é¢„å¤„ç†ï¼ˆç§»é™¤äº†ç“¦æ–¯æ¶Œå‡ºé‡ç›¸å…³å¤„ç†ï¼‰
        """
        logger.debug(f"æ•°æ®é¢„å¤„ç†å¼€å§‹ï¼ˆè®­ç»ƒæ¨¡å¼: {'æ˜¯' if is_training else 'å¦'}ï¼‰ï¼ŒåŸå§‹æ ·æœ¬: {len(data)}")
        # Step 1: ç»Ÿä¸€æ•°æ®æ ¼å¼ä¸ºDataFrame
        if isinstance(data, pd.DataFrame):
            df = data.copy()
        else:
            df = pd.DataFrame(data)
        # Step 2: æ·»åŠ æ—¶ç©ºå”¯ä¸€æ ‡è¯†ï¼ˆä»£æ›¿åŸæœ‰å»é‡é€»è¾‘ï¼‰
        df = self._add_spatiotemporal_identifier(df)
        # Step 3: æ£€æŸ¥å¹¶è¡¥å……å…³é”®æ—¶ç©ºç‰¹å¾
        df = self._enrich_spatiotemporal_features(df)
        # Step 4: è‡ªåŠ¨è¡¥å……æ–­å±‚å½±å“ç³»æ•°
        if 'fault_influence_strength' not in df.columns or df['fault_influence_strength'].isnull().any():
            logger.debug("æ£€æµ‹åˆ° fault_influence_strength ç¼ºå¤±ï¼Œè‡ªåŠ¨è®¡ç®—")
            if fault_calculator and db_utils:
                df_dict = fault_calculator.calculate_fault_influence_strength(df.to_dict('records'), db_utils)
                df = pd.DataFrame(df_dict)
            else:
                df['fault_influence_strength'] = 0.5
        # Step 5: æ ¡éªŒåŒºåŸŸæªæ–½å¼ºåº¦
        if 'regional_measure_strength' not in df.columns or df['regional_measure_strength'].isnull().any():
            raise ValueError(
                "æ•°æ®ç¼ºå°‘ regional_measure_strengthï¼éœ€å…ˆè°ƒç”¨ /api/model/calculate_regional_strength æ¥å£è®¡ç®—"
            )
        # Step 6: åˆ—åæ ‡å‡†åŒ–
        df.columns = (
            df.columns
            .astype(str)
            .str.strip()
            .str.replace(' ', '_')
            .str.replace('-', '_')
        )
        # Step 7: ä¿®æ­£å»é‡é€»è¾‘
        original_count = len(df)
        df = df.drop_duplicates(keep='first')
        if len(df) < original_count:
            logger.info(f"ç§»é™¤ {original_count - len(df)} æ¡å®Œå…¨ç›¸åŒçš„é‡å¤è®°å½•")
        # Step 8: æŒ‰æ—¶é—´å’Œç©ºé—´æ’åº
        if 'measurement_date' in df.columns and 'measurement_time' in df.columns:
            try:
                df['measurement_datetime'] = pd.to_datetime(
                    df['measurement_date'] + ' ' + df['measurement_time'].fillna('00:00:00')
                )
                df = df.sort_values(['working_face', 'measurement_datetime',
                                     'distance_from_entrance']).reset_index(drop=True)
                logger.debug("æŒ‰å·¥ä½œé¢ã€æµ‹é‡æ—¶é—´ã€è·å…¥å£è·ç¦»æ’åº")
            except Exception as e:
                logger.warning(f"æ—¶é—´æ’åºå¤±è´¥ï¼š{str(e)}")
        elif 'distance_from_entrance' in df.columns:
            df = df.sort_values('distance_from_entrance').reset_index(drop=True)
        # Step 9: ç¼ºå¤±å€¼å¡«å……
        # åˆ†ç±»ç‰¹å¾å¡«å……
        for col in self.base_categorical:
            if col in df.columns and df[col].isnull().any():
                fill_val = df[col].mode()[0] if not df[col].mode().empty else "æœªçŸ¥"
                df[col] = df[col].fillna(fill_val)
        # æ•°å€¼ç‰¹å¾å¡«å……
        for col in self.base_numeric:
            if col in df.columns and df[col].isnull().any():
                fill_val = df[col].median() if not df[col].isna().all() else 0.0
                df[col] = df[col].fillna(fill_val)
        # ç›®æ ‡ç‰¹å¾å¡«å……ï¼ˆè®­ç»ƒ/è¯„ä¼°æ—¶ç¡®ä¿æ— NaNï¼‰
        if is_training:
            for col in self.target_features:
                if col in df.columns and df[col].isnull().any():
                    fill_val = df[col].median() if not df[col].isna().all() else 0.0
                    df[col] = df[col].fillna(fill_val)
        # Step 10: ç”Ÿæˆæ—¶ç©ºç‰¹å¾ï¼ˆæ–°å¢ï¼‰
        df = self._generate_spatiotemporal_features(df)
        # Step 11: ç¡®ä¿æ‰€æœ‰æœŸæœ›ç‰¹å¾å­˜åœ¨
        # è·å–åŸºç¡€ç‰¹å¾åˆ—è¡¨
        base_features = self.base_categorical + self.base_numeric
        # ä½†æˆ‘ä»¬éœ€è¦æ‰€æœ‰å®é™…å­˜åœ¨äºdfä¸­çš„ç‰¹å¾
        all_features = list(df.columns)
        # ç§»é™¤ç›®æ ‡ç‰¹å¾å’Œéç‰¹å¾åˆ—
        non_feature_cols = self.target_features + ['_spatiotemporal_id', 'measurement_datetime']
        feature_cols = [col for col in all_features if col not in non_feature_cols]
        # ç¡®ä¿æ‰€æœ‰é…ç½®çš„ç‰¹å¾éƒ½å­˜åœ¨
        for col in base_features:
            if col not in df.columns:
                fill_val = "æœªçŸ¥" if col in self.base_categorical else 0.0
                df[col] = fill_val
                logger.debug(f"ç‰¹å¾ {col} ç¼ºå¤±ï¼Œå¡«å……é»˜è®¤å€¼ï¼š{fill_val}")
                if col not in feature_cols:
                    feature_cols.append(col)
        # Step 12: è®­ç»ƒ/é¢„æµ‹æ¨¡å¼å·®å¼‚åŒ–å¤„ç†
        if is_training:
            missing_targets = [t for t in self.target_features if t not in df.columns]
            if missing_targets:
                raise ValueError(f"è®­ç»ƒæ•°æ®ç¼ºå°‘ç›®æ ‡ç‰¹å¾ï¼š{missing_targets}")
            # ç¡®ä¿ç‰¹å¾åˆ—ä¸åŒ…å«ç›®æ ‡åˆ—
            feature_cols = [col for col in feature_cols if col not in self.target_features]
            logger.debug(f"è®­ç»ƒç‰¹å¾ç¡®å®šï¼šå…± {len(feature_cols)} ä¸ª")
            logger.debug(f"ç‰¹å¾åˆ—: {feature_cols}")
            # æ•°æ®è´¨é‡æ£€æŸ¥
            self._log_data_quality_summary(df)
            return df, feature_cols
        else:
            if not self.training_features:
                raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ³•ç¡®å®šé¢„æµ‹ç‰¹å¾é¡ºåº")
            keep_cols = self.training_features
            df = df[keep_cols]
            logger.debug(f"é¢„æµ‹æ•°æ®å¯¹é½ï¼šæŒ‰è®­ç»ƒç‰¹å¾é¡ºåºä¿ç•™ {len(df.columns)} ä¸ªå­—æ®µ")
            return df

    def _add_spatiotemporal_identifier(self, df):
        """
        æ·»åŠ æ—¶ç©ºå”¯ä¸€æ ‡è¯†ï¼Œé¿å…é”™è¯¯å»é‡
        """
        # ç”Ÿæˆå¤åˆå”¯ä¸€æ ‡è¯†
        identifier_parts = []
        # åŸºæœ¬ç©ºé—´æ ‡è¯†
        space_cols = ['working_face', 'x_coord', 'y_coord', 'z_coord']
        for col in space_cols:
            if col in df.columns:
                identifier_parts.append(col)
        # æ—¶é—´æ ‡è¯†ï¼ˆä¼˜å…ˆï¼‰
        if 'measurement_date' in df.columns:
            identifier_parts.append('measurement_date')
        if 'measurement_time' in df.columns:
            identifier_parts.append('measurement_time')
        # é’»å­”æ ‡è¯†
        if 'borehole_id' in df.columns:
            identifier_parts.append('borehole_id')
        if 'drilling_depth' in df.columns:
            identifier_parts.append('drilling_depth')
        # è·ç¦»æ ‡è¯†ï¼ˆå…³é”®ï¼‰
        if 'distance_to_face' in df.columns:
            identifier_parts.append('distance_to_face')
        elif 'face_advance_distance' in df.columns:
            identifier_parts.append('face_advance_distance')
        # ç”Ÿæˆå”¯ä¸€ID
        if identifier_parts:
            # æ£€æŸ¥è¿™äº›åˆ—æ˜¯å¦éƒ½åœ¨dfä¸­
            available_parts = [col for col in identifier_parts if col in df.columns]
            if available_parts:
                df['_spatiotemporal_id'] = df[available_parts].astype(str).agg('_'.join, axis=1)
            else:
                df['_spatiotemporal_id'] = df.index.astype(str)
        else:
            df['_spatiotemporal_id'] = df.index.astype(str)
        return df

    def _enrich_spatiotemporal_features(self, df):
        """
        è¡¥å……å…³é”®æ—¶ç©ºç‰¹å¾
        """
        # 1. è¡¥å……è·é‡‡é¢è·ç¦»ï¼ˆå¦‚ç¼ºå¤±ï¼‰
        if 'distance_to_face' not in df.columns:
            if 'face_advance_distance' in df.columns and 'drilling_depth' in df.columns:
                # ä¼°ç®—è·é‡‡é¢è·ç¦» = é’»å­”æ·±åº¦ + å·¥ä½œé¢æ¨è¿›è·ç¦»
                df['distance_to_face'] = df['drilling_depth'] + df['face_advance_distance'].fillna(0)
                logger.info("è‡ªåŠ¨è®¡ç®— distance_to_face ç‰¹å¾")
            else:
                df['distance_to_face'] = 0
                logger.warning("æ— æ³•è®¡ç®— distance_to_faceï¼Œè®¾ä¸º0")
        # 2. åˆ›å»ºæ—¶é—´åºåˆ—ç‰¹å¾
        if 'measurement_date' in df.columns:
            try:
                # è½¬æ¢ä¸ºæ—¶é—´æˆ³
                df['measurement_date_parsed'] = pd.to_datetime(df['measurement_date'])
                # è®¡ç®—æ—¶é—´åºåˆ—ç‰¹å¾
                df['days_since_start'] = (df['measurement_date_parsed'] -
                                          df['measurement_date_parsed'].min()).dt.days
                # æŒ‰å·¥ä½œé¢åˆ†ç»„çš„æ—¶é—´åºåˆ—
                if 'working_face' in df.columns:
                    df['days_in_workface'] = df.groupby('working_face')['measurement_date_parsed'].transform(
                        lambda x: (x - x.min()).dt.days
                    )
                logger.info("æ—¶é—´åºåˆ—ç‰¹å¾ç”Ÿæˆå®Œæˆ")
            except Exception as e:
                logger.warning(f"æ—¶é—´åºåˆ—ç‰¹å¾ç”Ÿæˆå¤±è´¥ï¼š{str(e)}")
        # 3. è®¡ç®—ç›¸é‚»æµ‹é‡çš„å˜åŒ–ç‡ï¼ˆç”¨äºæ£€æµ‹å¼‚å¸¸ï¼‰
        if 'distance_from_entrance' in df.columns and 'working_face' in df.columns:
            # å…ˆæŒ‰å·¥ä½œé¢å’Œè·ç¦»æ’åº
            df = df.sort_values(['working_face', 'distance_from_entrance']).reset_index(drop=True)

            # è®¡ç®—ç›¸é‚»qã€Så€¼çš„å˜åŒ–
            for target in ['gas_emission_q', 'drilling_cuttings_s', 'gas_emission_velocity_q']:
                if target in df.columns:
                    df[f'{target}_diff'] = df.groupby('working_face')[target].diff()
                    df[f'{target}_pct_change'] = df.groupby('working_face')[target].pct_change()
        return df

    def _generate_spatiotemporal_features(self, df):
        """
        ç”Ÿæˆæ—¶ç©ºäº¤äº’ç‰¹å¾
        """
        # 1. ç©ºé—´-æ—¶é—´äº¤äº’ç‰¹å¾
        if 'distance_to_face' in df.columns and 'days_since_start' in df.columns:
            df['distance_time_interaction'] = df['distance_to_face'] * df['days_since_start'] / 1000
        # 2. å·¥ä½œé¢æ¨è¿›ç‰¹å¾
        if 'face_advance_distance' in df.columns:
            # æ¨è¿›é€Ÿç‡ï¼ˆå¦‚æœ‰æ—¶é—´ä¿¡æ¯ï¼‰
            if 'measurement_date_parsed' in df.columns and 'working_face' in df.columns:
                # æŒ‰å·¥ä½œé¢åˆ†ç»„è®¡ç®—
                advance_rates = []
                for workface, group in df.groupby('working_face'):
                    group_sorted = group.sort_values('measurement_date_parsed')
                    rate = group_sorted['face_advance_distance'].diff() / (
                        group_sorted['measurement_date_parsed'].diff().dt.days.replace(0, 1e-9)
                    )
                    advance_rates.append(rate)
                # åˆå¹¶ç»“æœ
                df['advance_rate'] = pd.concat(advance_rates) if advance_rates else 0
        # 3. å†å²è¶‹åŠ¿ç‰¹å¾ï¼ˆåŒä¸€ä½ç½®çš„å†å²qã€Så€¼ï¼‰
        if 'x_coord' in df.columns and 'y_coord' in df.columns and 'z_coord' in df.columns:
            # åˆ›å»ºåæ ‡å“ˆå¸Œç”¨äºå¿«é€ŸåŒ¹é…
            df['coord_hash'] = (
                    df['x_coord'].round(1).astype(str) + '_' +
                    df['y_coord'].round(1).astype(str) + '_' +
                    df['z_coord'].round(1).astype(str)
            )
            # è®¡ç®—åŒä¸€åæ ‡ç‚¹çš„å†å²ç»Ÿè®¡
            for target in ['gas_emission_q', 'drilling_cuttings_s', 'gas_emission_velocity_q']:
                if target in df.columns:
                    # åŒä¸€åæ ‡ç‚¹çš„å†å²å¹³å‡å€¼
                    historical_mean = df.groupby('coord_hash')[target].expanding().mean().reset_index(level=0,
                                                                                                      drop=True)
                    df[f'{target}_historical_mean'] = historical_mean
                    # åŒä¸€åæ ‡ç‚¹çš„å˜åŒ–è¶‹åŠ¿
                    df[f'{target}_trend'] = df.groupby('coord_hash')[target].diff()
        return df

    def _log_data_quality_summary(self, df):
        """ç®€å•çš„æ•°æ®è´¨é‡æ‘˜è¦æ—¥å¿—"""
        try:
            # åŸºç¡€æ£€æŸ¥
            if hasattr(self, 'target_features'):
                for target in self.target_features:
                    if target in df.columns:
                        variance = df[target].var()
                        if variance < 0.1:
                            logger.warning(f"ğŸš¨ ç›®æ ‡ç‰¹å¾ {target} æ–¹å·®è¿‡ä½: {variance:.6f}")

            # ç¼ºå¤±å€¼æ£€æŸ¥
            missing_columns = df.columns[df.isnull().any()].tolist()
            if missing_columns:
                logger.warning(f"âš ï¸ æ•°æ®åŒ…å«ç¼ºå¤±å€¼çš„åˆ—: {missing_columns}")

        except Exception as e:
            logger.debug(f"æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {str(e)}")

    # def _calculate_coal_wall_emission(self, coal_thickness, tunneling_speed, initial_strength, roadway_length):
    #     """ç§æœ‰æ–¹æ³•ï¼šè®¡ç®—ç…¤å£ç“¦æ–¯æ¶Œå‡ºé‡ï¼ˆAQ1018â€”2006å…¬å¼ï¼‰"""
    #     try:
    #         if tunneling_speed <= 0:
    #             logger.warning("æ˜è¿›é€Ÿåº¦â‰¤0ï¼Œç…¤å£æ¶Œå‡ºé‡è®¾ä¸º0")
    #             return 0.0
    #         roadway_length = max(roadway_length, 0.0)
    #         val = coal_thickness * tunneling_speed * initial_strength * (
    #                 2 * np.sqrt(roadway_length / (tunneling_speed + 1e-9)) - 1
    #         )
    #         return max(0.0, float(val))
    #     except Exception as e:
    #         logger.error(f"è®¡ç®—ç…¤å£æ¶Œå‡ºé‡å¤±è´¥ï¼š{str(e)}", exc_info=True)
    #         return 0.0
    #
    # def _calculate_fallen_coal_emission(self, cross_section, coal_density, tunneling_speed, original_gas, residual_gas):
    #     """ç§æœ‰æ–¹æ³•ï¼šè®¡ç®—è½ç…¤ç“¦æ–¯æ¶Œå‡ºé‡ï¼ˆAQ1018â€”2006å…¬å¼ï¼‰"""
    #     try:
    #         gas_diff = max(0.0, (original_gas or 0.0) - (residual_gas or 0.0))
    #         val = cross_section * coal_density * tunneling_speed * gas_diff
    #         return max(0.0, float(val))
    #     except Exception as e:
    #         logger.error(f"è®¡ç®—è½ç…¤æ¶Œå‡ºé‡å¤±è´¥ï¼š{str(e)}", exc_info=True)
    #         return 0.0